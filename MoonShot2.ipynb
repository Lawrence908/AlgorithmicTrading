{"cells":[{"cell_type":"markdown","id":"1e866085-c160-4028-babd-0b2ca9b9fdba","metadata":{"id":"1e866085-c160-4028-babd-0b2ca9b9fdba"},"source":["# Project: MoonShot - AI-powered Trading Strategy\n","\n","This notebook outlines the development of MoonShot, an artificial intelligence (AI) system designed to implement and learn from a trading strategy. We aim to achieve the following objectives:\n","\n","---\n","\n","### Define a Trading Strategy:\n","We will establish a set of rules and indicators that MoonShot will use to identify and execute potential trades. This strategy could involve technical analysis, fundamental analysis, or a combination of both.\n","\n","## Trading Strategy:\n","The MoonShot trading strategy use a set of fundamentals and technical indicators to create buy/sell signals. The following is the list of datapoints that MoonShot uses:\n","\n","* Z score\n","* Bollinger Bands x RSI\n","* Simple Moving Averages\n","\n","### Implement the Strategy:\n","We will translate the defined trading strategy into code, enabling MoonShot to autonomously analyze market data, generate trading signals, and potentially execute trades (with proper safeguards in place).\n","\n","---\n","\n","### Train a Neural Network:\n","We will train a multilayer neural network on historical market data and the corresponding trading signals generated by MoonShot's strategy. This neural network will attempt to learn and potentially improve upon the initial strategy, potentially identifying new patterns or refining existing ones.\n","\n","We will consider the model a success if it is able to increase both the Win rate by 15% while maintaining the profit percentage.\n","\n","#### Throughout this notebook, we will document the development process, including:\n","\n","- Data acquisition and preparation\n","- Feature engineering and selection\n","- Building and training the neural network\n","- Evaluating the performance of MoonShot's strategy and the trained neural network\n","\n","---\n","\n","Disclaimer: This project is for educational purposes only and should not be used for real-world trading without proper risk management and regulatory compliance. The market is inherently risky, and any trading strategy, including those involving AI, is susceptible to losses."]},{"cell_type":"markdown","id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f","metadata":{"id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f"},"source":["# Import Required Libraries"]},{"cell_type":"code","execution_count":null,"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc","metadata":{"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc"},"outputs":[],"source":["# Required libraries\n","\n","# Import the generic libraries\n","import sys\n","import pytictoc\n","\n","#Import the neural network architecture\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","#Import financial data\n","import ta\n","import yfinance as yf\n","\n","# Import data science tools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pandas_datareader as pdr\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns"]},{"cell_type":"markdown","id":"0098c760-272a-4988-9d91-c640825df298","metadata":{"id":"0098c760-272a-4988-9d91-c640825df298"},"source":["---\n","# Load and Preprocess Data"]},{"cell_type":"markdown","id":"fe3618cf-3751-41de-911a-b82c1efa766c","metadata":{"id":"fe3618cf-3751-41de-911a-b82c1efa766c"},"source":["> This step starts with importing the dataset.\n","Next, we preprocess the data to ensure its readiness for training.\n","This includes cleaning the data to address missing values or outliers, normalizing or scaling features for consistency, and partitioning the data into training, validation, and test sets for effective model training and evaluation.\n","This careful preprocessing guarantees that the data is appropriately formatted for training our deep neural network, thus enhancing its performance and generalization capabilities."]},{"cell_type":"markdown","id":"faeae68d-c55e-43f5-b1b9-910034b34c61","metadata":{"id":"faeae68d-c55e-43f5-b1b9-910034b34c61"},"source":["## 1. import the dataset"]},{"cell_type":"code","execution_count":null,"id":"ec575afd-a525-4bb4-a2e8-e5f96b83f14e","metadata":{"id":"ec575afd-a525-4bb4-a2e8-e5f96b83f14e","outputId":"959bc15e-4406-4f6c-98fa-e7e3f781dd0b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Buy P/E Ratio</th>\n","      <th>Buy Fwd P/E Ratio</th>\n","      <th>Buy P/B Ratio</th>\n","      <th>Buy RSI</th>\n","      <th>Buy Upper BB</th>\n","      <th>Buy Bollinger %b</th>\n","      <th>Buy Lower BB</th>\n","      <th>Buy vol</th>\n","      <th>Buy Z Score</th>\n","      <th>Buy MACD</th>\n","      <th>Buy VWAP</th>\n","      <th>Buy OBV</th>\n","      <th>Buy Stoch</th>\n","      <th>Buy Awesome Oscillator</th>\n","      <th>Buy Ultimate Oscillator</th>\n","      <th>Buy TSI</th>\n","      <th>Buy Acum/Dist</th>\n","      <th>Profitable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>24.211988</td>\n","      <td>271.219748</td>\n","      <td>-0.048257</td>\n","      <td>251.430252</td>\n","      <td>4.947374</td>\n","      <td>-2.137498</td>\n","      <td>-2.154089</td>\n","      <td>261.214801</td>\n","      <td>-2618000</td>\n","      <td>1.950719</td>\n","      <td>1.286618</td>\n","      <td>41.995321</td>\n","      <td>-1.470883</td>\n","      <td>3.717088e+05</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>43.786890</td>\n","      <td>91.902800</td>\n","      <td>0.220007</td>\n","      <td>82.260200</td>\n","      <td>2.410650</td>\n","      <td>-1.091614</td>\n","      <td>-0.277936</td>\n","      <td>86.380876</td>\n","      <td>519400</td>\n","      <td>35.112060</td>\n","      <td>-6.482647</td>\n","      <td>47.008240</td>\n","      <td>-15.532739</td>\n","      <td>-3.451807e+06</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16.743890</td>\n","      <td>11.495973</td>\n","      <td>1.871449</td>\n","      <td>20.430433</td>\n","      <td>153.330658</td>\n","      <td>-0.101009</td>\n","      <td>140.353342</td>\n","      <td>3.244329</td>\n","      <td>-2.343166</td>\n","      <td>-1.432220</td>\n","      <td>145.745994</td>\n","      <td>-564100</td>\n","      <td>8.786611</td>\n","      <td>-1.082588</td>\n","      <td>38.515067</td>\n","      <td>2.319048</td>\n","      <td>1.118320e+06</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12.446736</td>\n","      <td>7.277964</td>\n","      <td>2.530213</td>\n","      <td>24.356451</td>\n","      <td>199.953365</td>\n","      <td>-0.036277</td>\n","      <td>173.050635</td>\n","      <td>6.725683</td>\n","      <td>-2.090792</td>\n","      <td>-1.231614</td>\n","      <td>185.151154</td>\n","      <td>-5783400</td>\n","      <td>11.827957</td>\n","      <td>-12.969765</td>\n","      <td>37.629497</td>\n","      <td>-25.499065</td>\n","      <td>-1.259635e+07</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>34.833874</td>\n","      <td>20.529467</td>\n","      <td>8.169850</td>\n","      <td>43.106665</td>\n","      <td>174.727510</td>\n","      <td>0.288348</td>\n","      <td>158.774490</td>\n","      <td>3.988255</td>\n","      <td>-0.825173</td>\n","      <td>-0.567193</td>\n","      <td>171.210284</td>\n","      <td>-5316300</td>\n","      <td>0.300120</td>\n","      <td>-0.592059</td>\n","      <td>-6.517047</td>\n","      <td>-6.758611</td>\n","      <td>-9.577518e+07</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2378</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>26.240493</td>\n","      <td>14.798538</td>\n","      <td>0.006185</td>\n","      <td>13.057462</td>\n","      <td>0.435269</td>\n","      <td>-1.925247</td>\n","      <td>-0.201803</td>\n","      <td>13.970538</td>\n","      <td>155505900</td>\n","      <td>11.235955</td>\n","      <td>0.151353</td>\n","      <td>34.518671</td>\n","      <td>10.070177</td>\n","      <td>-1.702076e+08</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2379</th>\n","      <td>33.582000</td>\n","      <td>20.858385</td>\n","      <td>4.872040</td>\n","      <td>11.726526</td>\n","      <td>128.535788</td>\n","      <td>-0.207307</td>\n","      <td>113.397212</td>\n","      <td>3.784644</td>\n","      <td>-2.757591</td>\n","      <td>-1.646504</td>\n","      <td>119.362273</td>\n","      <td>74241800</td>\n","      <td>6.334311</td>\n","      <td>-2.076471</td>\n","      <td>31.686509</td>\n","      <td>-3.626917</td>\n","      <td>4.712295e+07</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2380</th>\n","      <td>48.500000</td>\n","      <td>115.476190</td>\n","      <td>2.465977</td>\n","      <td>49.989695</td>\n","      <td>137.367120</td>\n","      <td>0.424863</td>\n","      <td>130.969880</td>\n","      <td>1.599310</td>\n","      <td>-0.292939</td>\n","      <td>-0.466962</td>\n","      <td>133.911666</td>\n","      <td>23991500</td>\n","      <td>57.364341</td>\n","      <td>-2.311706</td>\n","      <td>48.990757</td>\n","      <td>-1.053302</td>\n","      <td>1.962370e+07</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2381</th>\n","      <td>9.650743</td>\n","      <td>10.323895</td>\n","      <td>2.104130</td>\n","      <td>40.495837</td>\n","      <td>137.930515</td>\n","      <td>0.150069</td>\n","      <td>127.419485</td>\n","      <td>2.627758</td>\n","      <td>-1.364281</td>\n","      <td>-1.142085</td>\n","      <td>132.167718</td>\n","      <td>27058200</td>\n","      <td>21.522310</td>\n","      <td>-1.201559</td>\n","      <td>43.661236</td>\n","      <td>5.598968</td>\n","      <td>-1.785873e+07</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2382</th>\n","      <td>11.586612</td>\n","      <td>8.452992</td>\n","      <td>0.989198</td>\n","      <td>22.937405</td>\n","      <td>134.033689</td>\n","      <td>-0.048616</td>\n","      <td>124.694311</td>\n","      <td>2.334845</td>\n","      <td>-2.138900</td>\n","      <td>-1.570617</td>\n","      <td>129.382955</td>\n","      <td>-12810000</td>\n","      <td>10.335449</td>\n","      <td>1.586118</td>\n","      <td>35.616269</td>\n","      <td>20.668576</td>\n","      <td>-2.358078e+09</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2383 rows × 18 columns</p>\n","</div>"],"text/plain":["      Buy P/E Ratio  Buy Fwd P/E Ratio  Buy P/B Ratio    Buy RSI  \\\n","0          0.000000           0.000000       0.000000  24.211988   \n","1          0.000000           0.000000       0.000000  43.786890   \n","2         16.743890          11.495973       1.871449  20.430433   \n","3         12.446736           7.277964       2.530213  24.356451   \n","4         34.833874          20.529467       8.169850  43.106665   \n","...             ...                ...            ...        ...   \n","2378       0.000000           0.000000       0.000000  26.240493   \n","2379      33.582000          20.858385       4.872040  11.726526   \n","2380      48.500000         115.476190       2.465977  49.989695   \n","2381       9.650743          10.323895       2.104130  40.495837   \n","2382      11.586612           8.452992       0.989198  22.937405   \n","\n","      Buy Upper BB  Buy Bollinger %b  Buy Lower BB   Buy vol  Buy Z Score  \\\n","0       271.219748         -0.048257    251.430252  4.947374    -2.137498   \n","1        91.902800          0.220007     82.260200  2.410650    -1.091614   \n","2       153.330658         -0.101009    140.353342  3.244329    -2.343166   \n","3       199.953365         -0.036277    173.050635  6.725683    -2.090792   \n","4       174.727510          0.288348    158.774490  3.988255    -0.825173   \n","...            ...               ...           ...       ...          ...   \n","2378     14.798538          0.006185     13.057462  0.435269    -1.925247   \n","2379    128.535788         -0.207307    113.397212  3.784644    -2.757591   \n","2380    137.367120          0.424863    130.969880  1.599310    -0.292939   \n","2381    137.930515          0.150069    127.419485  2.627758    -1.364281   \n","2382    134.033689         -0.048616    124.694311  2.334845    -2.138900   \n","\n","      Buy MACD    Buy VWAP    Buy OBV  Buy Stoch  Buy Awesome Oscillator  \\\n","0    -2.154089  261.214801   -2618000   1.950719                1.286618   \n","1    -0.277936   86.380876     519400  35.112060               -6.482647   \n","2    -1.432220  145.745994    -564100   8.786611               -1.082588   \n","3    -1.231614  185.151154   -5783400  11.827957              -12.969765   \n","4    -0.567193  171.210284   -5316300   0.300120               -0.592059   \n","...        ...         ...        ...        ...                     ...   \n","2378 -0.201803   13.970538  155505900  11.235955                0.151353   \n","2379 -1.646504  119.362273   74241800   6.334311               -2.076471   \n","2380 -0.466962  133.911666   23991500  57.364341               -2.311706   \n","2381 -1.142085  132.167718   27058200  21.522310               -1.201559   \n","2382 -1.570617  129.382955  -12810000  10.335449                1.586118   \n","\n","      Buy Ultimate Oscillator    Buy TSI  Buy Acum/Dist Profitable  \n","0                   41.995321  -1.470883   3.717088e+05        Yes  \n","1                   47.008240 -15.532739  -3.451807e+06        Yes  \n","2                   38.515067   2.319048   1.118320e+06        Yes  \n","3                   37.629497 -25.499065  -1.259635e+07        Yes  \n","4                   -6.517047  -6.758611  -9.577518e+07        Yes  \n","...                       ...        ...            ...        ...  \n","2378                34.518671  10.070177  -1.702076e+08        Yes  \n","2379                31.686509  -3.626917   4.712295e+07        Yes  \n","2380                48.990757  -1.053302   1.962370e+07        Yes  \n","2381                43.661236   5.598968  -1.785873e+07        Yes  \n","2382                35.616269  20.668576  -2.358078e+09        Yes  \n","\n","[2383 rows x 18 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["buy_trades_core = pd.read_csv(\"CSV/buytable.csv\")\n","buy_trades_core.fillna(0)"]},{"cell_type":"markdown","id":"112cebde-ab83-4697-b412-0748cc885f54","metadata":{"id":"112cebde-ab83-4697-b412-0748cc885f54"},"source":["### preprocess the main dataset\n","\n","* Drop data not required by any subsets\n","* Split the data into the X and Y sets\n","* Handle missing data\n","* scaling, normalization, and correlations"]},{"cell_type":"code","execution_count":null,"id":"f5c0eb50-4460-4040-8738-2274aeb1671c","metadata":{"id":"f5c0eb50-4460-4040-8738-2274aeb1671c","outputId":"296087bb-e0ab-492e-e5a6-03394062dae9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/41/wjq30yh97cv8pn434b5z6bt40000gn/T/ipykernel_63084/2750544182.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"No\", 0)\n"]},{"data":{"text/plain":["(1906, 16)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# We start by changing the string values for the profitable column into their boolean equivalents.\n","# This column will become out target (y) values\n","buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"Yes\", 1)\n","buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"No\", 0)\n","\n","# Removing columns that are unused for future subsets.\n","# Columns are removed upstream to avoid corrupting future scaling, normalization, or correlations with bad data.\n","buy_trades_core = buy_trades_core.drop(columns= [\"Buy vol\"])\n","buy_trades = buy_trades_core\n","\n","# Two new dataframes are created, one contains all of the input features of the dataset (x), the other contains the target values (y)\n","buy_x = buy_trades.drop(columns= \"Profitable\")\n","buy_y = buy_trades[\"Profitable\"]\n","\n","# The input features are then preprocessed using standard scaling and normalization techniques.\n","# Scaling helps prevent feature domination in model training and increases convergence in the gradient descent used in optimization functions\n","# The scaler is initialized from the scikit learn library and then fit to the features of our dataset\n","scaler = StandardScaler()\n","scaler.fit(buy_x)\n","# We finalize the process by applying the scaler to the data in our dataframe. This is stored as a numpy array.\n","buy_x_scaled = scaler.transform(buy_x)\n","\n","# The dataset is split into the training and test sets.\n","# Data is shuffled to prevent overfitting to subsets and reduce underlying patterns in time based data.\n","# We use the industry standard of starting with an 80/20 split on the data set, adjusting if needed based on task complexity and set size\n","buy_x_train, buy_x_test, buy_y_train, buy_y_test = train_test_split(buy_x_scaled, buy_y, test_size=0.2, random_state = 42)\n","\n","buy_x_train.shape"]},{"cell_type":"code","execution_count":null,"id":"4252d0d0-d2f1-45fb-b6ea-33f64ad4937e","metadata":{"id":"4252d0d0-d2f1-45fb-b6ea-33f64ad4937e"},"outputs":[],"source":["\n","# We define the class of a simple Neural Network through the use of the PyTorch library\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SimpleNN, self).__init__()\n","        #define the first layer which has neurons = <input_size> with edges per neuron = <hidden_size>\n","        self.fc0 = nn.Linear(input_size, hidden_size)\n","        #defines the hidden layers with <hidden_size> neurons and <hidden_size> outgoing edges\n","        self.fc1 = nn.Linear(hidden_size, hidden_size)\n","        # defines the second hidden layer\n","        #self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        # defines the third hidden layer\n","        #self.fc3 = nn.Linear(hidden_size, hidden_size)\n","        #defines the output layer with hidden_size connections going to output_size neurons\n","        self.fcf = nn.Linear(hidden_size, output_size)\n","        #defines the relu function used as the activation function between neurons\n","        self.relu = nn.ReLU()\n","        #defines the final function used on the forward pass\n","        self.sigmoid = nn.Sigmoid()\n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, x):\n","        x = self.fc0(x)\n","        x = self.relu(x)\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","\n","        #x = self.fc2(x) # Second hidden layer\n","       # x = self.relu(x)\n","\n","       # x = self.fc3(x) # Third hidden layer\n","       # x = self.relu(x)\n","\n","        x = self.fcf(x)\n","        x = self.sigmoid(x)\n","        #x = self.softmax(x)\n","        #x = x.squeeze(1) # added to remove additional dimension [400, 1] added during pytorch linear layering removed for softmax\n","        return x\n","\n","## Added a second layer with lines\n","#         self.fc2a = nn.Linear(hidden_size, hidden_size)\n","#        x = self.relu(x)\n","#        x = self.fc2a(x)"]},{"cell_type":"markdown","id":"a33a8e8b-45b2-4a18-b842-693674d3009b","metadata":{"id":"a33a8e8b-45b2-4a18-b842-693674d3009b"},"source":["## Training Setup"]},{"cell_type":"markdown","id":"47b0341d-252c-4782-9705-385ef965fbf3","metadata":{"id":"47b0341d-252c-4782-9705-385ef965fbf3"},"source":["> We attempt to convert the numpy and pandas series we have currently used for our dataset into tensors\n","Pandas dataframes and Numpy Arrays are used before this step for data exploration and manipulation but the deep learning library pytorch performs operations on tensors."]},{"cell_type":"code","execution_count":null,"id":"fe398dd5-541e-4e98-a917-35e2242ec792","metadata":{"id":"fe398dd5-541e-4e98-a917-35e2242ec792","outputId":"6c07704b-f1a7-4293-9019-42d3baf0323a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Created Y train tensor\n","Created Y validation tensor\n","Created X train tensor\n"]}],"source":["\n","try:\n","    buy_y_train_tensor = torch.from_numpy(buy_y_train.values)\n","    buy_y_train_tensor = buy_y_train_tensor.float()\n","    print(\"Created Y train tensor\")\n","    buy_y_validation_tensor = torch.from_numpy(buy_y_test.values)\n","    buy_y_validation_tensor = buy_y_validation_tensor.float()\n","    print(\"Created Y validation tensor\")\n","except ValueError:\n","    print(\"Error: buy_y_train or buy_y_test contains non-convertible values.\")\n","try:\n","    buy_x_train_tensor = torch.from_numpy(buy_x_train)\n","    buy_x_train_tensor = buy_x_train_tensor.float()\n","    print(\"Created X train tensor\")\n","    print(\"Createc X validation tensor\")\n","    buy_x_validation_tensor = torch.from_numpy(buy_x_test)\n","    buy_x_validation_tensor = buy_x_validation_tensor.float()\n","except ValueError:\n","    print(\"Error: buy_x_train contains non-convertible values.\")\n","\n"]},{"cell_type":"markdown","id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508","metadata":{"id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508"},"source":["## Training and Tuning"]},{"cell_type":"markdown","id":"8rnk3fyyNbB4","metadata":{"id":"8rnk3fyyNbB4"},"source":["> Here we define the hyperparameters of the neural network and begin training the network with those parameters. As deep learning is an iterative process- with model degredation and improvements both contributing to overall progress- this section does not contain the history of experimental training and parameter tuning that moonShot has undergone."]},{"cell_type":"code","execution_count":null,"id":"ea6d6453-bed0-418b-a6f3-681f0c44efc9","metadata":{"id":"ea6d6453-bed0-418b-a6f3-681f0c44efc9"},"outputs":[],"source":["\n","input_size = len(buy_x_train_tensor[0])\n","hidden_size = 56\n","#56\n","output_size = 1\n","learning_rate = 0.00001\n","num_epochs = 100\n","\n","# Loss: 0.4661\n","# Loss with 2 layers: 0.4745\n"]},{"cell_type":"code","execution_count":null,"id":"6bece210-a088-40d4-8394-96388947100f","metadata":{"id":"6bece210-a088-40d4-8394-96388947100f"},"outputs":[],"source":["moonShot_buy = SimpleNN(input_size, hidden_size, output_size)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(moonShot_buy.parameters(), lr=learning_rate)\n","# optimizer = torch.optim.SGD(moonShot_buy.parameters(), lr=learning_rate, momentum=0.9)\n","# optimizer = torch.optim.Adagrad(moonShot_buy.parameters(), lr=learning_rate)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"50f7c5f7-070e-43f2-b017-74bab65881e1","metadata":{"id":"50f7c5f7-070e-43f2-b017-74bab65881e1"},"outputs":[],"source":["\n","def evaluate(model, x_val, y_val):\n","  \"\"\"\n","  This function evaluates the model performance on a validation set.\n","\n","  Args:\n","      model: The deep neural network model.\n","      x_val: Validation set input data.\n","      y_val: Validation set target labels.\n","\n","  Returns:\n","      val_loss: The validation loss (calculated using the criterion function).\n","      val_accuracy: The validation accuracy.\n","  \"\"\"\n","  with torch.no_grad():  # Deactivate gradient calculation for validation\n","    # Forward pass on validation set\n","    val_outputs = model(x_val)\n","    val_loss = criterion(val_outputs, y_val)\n","\n","    # Calculate accuracy\n","    predicted = (val_outputs > 0.5).float()  # Thresholding for binary classification\n","    val_accuracy = (predicted == y_val).sum() / len(y_val)\n","\n","  return val_loss.item(), val_accuracy.item()\n"]},{"cell_type":"code","execution_count":null,"id":"420dea90-74ee-40cd-a36f-c3ab34a05ec4","metadata":{"id":"420dea90-74ee-40cd-a36f-c3ab34a05ec4","outputId":"b4be6f80-a839-448c-ed80-3a47f0fd21bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [2/100], Train Loss: 0.6539, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [4/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [6/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [8/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [10/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [12/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [14/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [16/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [18/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [20/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [22/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [24/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [26/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6080\n","Epoch [28/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [30/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [32/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [34/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [36/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [38/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [40/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [42/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [44/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [46/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [48/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [50/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [52/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [54/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [56/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [58/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [60/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [62/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [64/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [66/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [68/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [70/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [72/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [74/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [76/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [78/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [80/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [82/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [84/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [86/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [88/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [90/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [92/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [94/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [96/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [98/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [100/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6122\n"]}],"source":["# Reshape target tensor to match output shape\n","buy_y_train_tensor = buy_y_train_tensor.view(-1, 1)\n","buy_y_validation_tensor = buy_y_validation_tensor.view(-1, 1)\n","\n","for epoch in range(num_epochs):\n","    moonShot_buy.train()\n","    optimizer.zero_grad()\n","\n","# The forward pass as defined in the neural network architecture\n","    outputs = moonShot_buy(buy_x_train_tensor)\n","    loss = criterion(outputs, buy_y_train_tensor)\n","\n","# Backward pass of the calculated loss\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","# Evaluate on validation set\n","    val_loss, val_accuracy = evaluate(moonShot_buy, buy_x_validation_tensor, buy_y_validation_tensor)\n","\n","# Print loss and validation metrics (optional)\n","    if (epoch + 1) % 2 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":null,"id":"c53146ea-d721-4bcb-a763-8cec88f6fae0","metadata":{"id":"c53146ea-d721-4bcb-a763-8cec88f6fae0"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
