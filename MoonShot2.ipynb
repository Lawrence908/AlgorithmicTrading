{"cells":[{"cell_type":"markdown","id":"1e866085-c160-4028-babd-0b2ca9b9fdba","metadata":{"id":"1e866085-c160-4028-babd-0b2ca9b9fdba"},"source":["# Project: MoonShot - AI-powered Trading Strategy\n","\n","This notebook outlines the development of MoonShot, an artificial intelligence (AI) system designed to implement and learn from a trading strategy. We aim to achieve the following objectives:\n","\n","---\n","\n","### Define a Trading Strategy:\n","We will establish a set of rules and indicators that MoonShot will use to identify and execute potential trades. This strategy could involve technical analysis, fundamental analysis, or a combination of both.\n","\n","## Trading Strategy:\n","The MoonShot trading strategy use a set of fundamentals and technical indicators to create buy/sell signals. The following is the list of datapoints that MoonShot uses:\n","\n","* Z score\n","* Bollinger Bands x RSI\n","* Simple Moving Averages\n","\n","### Implement the Strategy:\n","We will translate the defined trading strategy into code, enabling MoonShot to autonomously analyze market data, generate trading signals, and potentially execute trades (with proper safeguards in place).\n","\n","---\n","\n","### Train a Neural Network:\n","We will train a multilayer neural network on historical market data and the corresponding trading signals generated by MoonShot's strategy. This neural network will attempt to learn and potentially improve upon the initial strategy, potentially identifying new patterns or refining existing ones.\n","\n","We will consider the model a success if it is able to increase both the Win rate by 15% while maintaining the profit percentage.\n","\n","#### Throughout this notebook, we will document the development process, including:\n","\n","- Data acquisition and preparation\n","- Feature engineering and selection\n","- Building and training the neural network\n","- Evaluating the performance of MoonShot's strategy and the trained neural network\n","\n","---\n","\n","Disclaimer: This project is for educational purposes only and should not be used for real-world trading without proper risk management and regulatory compliance. The market is inherently risky, and any trading strategy, including those involving AI, is susceptible to losses."]},{"cell_type":"markdown","id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f","metadata":{"id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f"},"source":["# Import Required Libraries"]},{"cell_type":"code","execution_count":1,"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc","metadata":{"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc"},"outputs":[],"source":["# Required libraries\n","\n","# Import the generic libraries\n","import sys\n","import pytictoc\n","\n","#Import the neural network architecture\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","#Import financial data\n","import ta\n","import yfinance as yf\n","\n","# Import data science tools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pandas_datareader as pdr\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns"]},{"cell_type":"markdown","id":"0098c760-272a-4988-9d91-c640825df298","metadata":{"id":"0098c760-272a-4988-9d91-c640825df298"},"source":["---\n","# Load and Preprocess Data"]},{"cell_type":"markdown","id":"fe3618cf-3751-41de-911a-b82c1efa766c","metadata":{"id":"fe3618cf-3751-41de-911a-b82c1efa766c"},"source":["> This step starts with importing the dataset.\n","Next, we preprocess the data to ensure its readiness for training.\n","This includes cleaning the data to address missing values or outliers, normalizing or scaling features for consistency, and partitioning the data into training, validation, and test sets for effective model training and evaluation.\n","This careful preprocessing guarantees that the data is appropriately formatted for training our deep neural network, thus enhancing its performance and generalization capabilities."]},{"cell_type":"markdown","id":"faeae68d-c55e-43f5-b1b9-910034b34c61","metadata":{"id":"faeae68d-c55e-43f5-b1b9-910034b34c61"},"source":["## 1. import the dataset"]},{"cell_type":"code","execution_count":2,"id":"ec575afd-a525-4bb4-a2e8-e5f96b83f14e","metadata":{"id":"ec575afd-a525-4bb4-a2e8-e5f96b83f14e","outputId":"959bc15e-4406-4f6c-98fa-e7e3f781dd0b"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'CSV/buytable.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m buy_trades_core \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCSV/buytable.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m buy_trades_core\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m~/AlgorithmicTrading/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/AlgorithmicTrading/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m~/AlgorithmicTrading/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/AlgorithmicTrading/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m~/AlgorithmicTrading/.venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CSV/buytable.csv'"]}],"source":["buy_trades_core = pd.read_csv(\"CSV/buytable.csv\")\n","buy_trades_core.fillna(0)"]},{"cell_type":"markdown","id":"112cebde-ab83-4697-b412-0748cc885f54","metadata":{"id":"112cebde-ab83-4697-b412-0748cc885f54"},"source":["### preprocess the main dataset\n","\n","* Drop data not required by any subsets\n","* Split the data into the X and Y sets\n","* Handle missing data\n","* scaling, normalization, and correlations"]},{"cell_type":"code","execution_count":null,"id":"f5c0eb50-4460-4040-8738-2274aeb1671c","metadata":{"id":"f5c0eb50-4460-4040-8738-2274aeb1671c","outputId":"296087bb-e0ab-492e-e5a6-03394062dae9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/41/wjq30yh97cv8pn434b5z6bt40000gn/T/ipykernel_63084/2750544182.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"No\", 0)\n"]},{"data":{"text/plain":["(1906, 16)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# We start by changing the string values for the profitable column into their boolean equivalents.\n","# This column will become out target (y) values\n","buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"Yes\", 1)\n","buy_trades_core[\"Profitable\"] = buy_trades_core[\"Profitable\"].replace(\"No\", 0)\n","\n","# Removing columns that are unused for future subsets.\n","# Columns are removed upstream to avoid corrupting future scaling, normalization, or correlations with bad data.\n","buy_trades_core = buy_trades_core.drop(columns= [\"Buy vol\"])\n","buy_trades = buy_trades_core\n","\n","# Two new dataframes are created, one contains all of the input features of the dataset (x), the other contains the target values (y)\n","buy_x = buy_trades.drop(columns= \"Profitable\")\n","buy_y = buy_trades[\"Profitable\"]\n","\n","# The input features are then preprocessed using standard scaling and normalization techniques.\n","# Scaling helps prevent feature domination in model training and increases convergence in the gradient descent used in optimization functions\n","# The scaler is initialized from the scikit learn library and then fit to the features of our dataset\n","scaler = StandardScaler()\n","scaler.fit(buy_x)\n","# We finalize the process by applying the scaler to the data in our dataframe. This is stored as a numpy array.\n","buy_x_scaled = scaler.transform(buy_x)\n","\n","# The dataset is split into the training and test sets.\n","# Data is shuffled to prevent overfitting to subsets and reduce underlying patterns in time based data.\n","# We use the industry standard of starting with an 80/20 split on the data set, adjusting if needed based on task complexity and set size\n","buy_x_train, buy_x_test, buy_y_train, buy_y_test = train_test_split(buy_x_scaled, buy_y, test_size=0.2, random_state = 42)\n","\n","buy_x_train.shape"]},{"cell_type":"code","execution_count":null,"id":"4252d0d0-d2f1-45fb-b6ea-33f64ad4937e","metadata":{"id":"4252d0d0-d2f1-45fb-b6ea-33f64ad4937e"},"outputs":[],"source":["\n","# We define the class of a simple Neural Network through the use of the PyTorch library\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SimpleNN, self).__init__()\n","        #define the first layer which has neurons = <input_size> with edges per neuron = <hidden_size>\n","        self.fc0 = nn.Linear(input_size, hidden_size)\n","        #defines the hidden layers with <hidden_size> neurons and <hidden_size> outgoing edges\n","        self.fc1 = nn.Linear(hidden_size, hidden_size)\n","        # defines the second hidden layer\n","        #self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        # defines the third hidden layer\n","        #self.fc3 = nn.Linear(hidden_size, hidden_size)\n","        #defines the output layer with hidden_size connections going to output_size neurons\n","        self.fcf = nn.Linear(hidden_size, output_size)\n","        #defines the relu function used as the activation function between neurons\n","        self.relu = nn.ReLU()\n","        #defines the final function used on the forward pass\n","        self.sigmoid = nn.Sigmoid()\n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, x):\n","        x = self.fc0(x)\n","        x = self.relu(x)\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","\n","        #x = self.fc2(x) # Second hidden layer\n","       # x = self.relu(x)\n","\n","       # x = self.fc3(x) # Third hidden layer\n","       # x = self.relu(x)\n","\n","        x = self.fcf(x)\n","        x = self.sigmoid(x)\n","        #x = self.softmax(x)\n","        #x = x.squeeze(1) # added to remove additional dimension [400, 1] added during pytorch linear layering removed for softmax\n","        return x\n","\n","## Added a second layer with lines\n","#         self.fc2a = nn.Linear(hidden_size, hidden_size)\n","#        x = self.relu(x)\n","#        x = self.fc2a(x)"]},{"cell_type":"markdown","id":"a33a8e8b-45b2-4a18-b842-693674d3009b","metadata":{"id":"a33a8e8b-45b2-4a18-b842-693674d3009b"},"source":["## Training Setup"]},{"cell_type":"markdown","id":"47b0341d-252c-4782-9705-385ef965fbf3","metadata":{"id":"47b0341d-252c-4782-9705-385ef965fbf3"},"source":["> We attempt to convert the numpy and pandas series we have currently used for our dataset into tensors\n","Pandas dataframes and Numpy Arrays are used before this step for data exploration and manipulation but the deep learning library pytorch performs operations on tensors."]},{"cell_type":"code","execution_count":null,"id":"fe398dd5-541e-4e98-a917-35e2242ec792","metadata":{"id":"fe398dd5-541e-4e98-a917-35e2242ec792","outputId":"6c07704b-f1a7-4293-9019-42d3baf0323a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Created Y train tensor\n","Created Y validation tensor\n","Created X train tensor\n"]}],"source":["\n","try:\n","    buy_y_train_tensor = torch.from_numpy(buy_y_train.values)\n","    buy_y_train_tensor = buy_y_train_tensor.float()\n","    print(\"Created Y train tensor\")\n","    buy_y_validation_tensor = torch.from_numpy(buy_y_test.values)\n","    buy_y_validation_tensor = buy_y_validation_tensor.float()\n","    print(\"Created Y validation tensor\")\n","except ValueError:\n","    print(\"Error: buy_y_train or buy_y_test contains non-convertible values.\")\n","try:\n","    buy_x_train_tensor = torch.from_numpy(buy_x_train)\n","    buy_x_train_tensor = buy_x_train_tensor.float()\n","    print(\"Created X train tensor\")\n","    print(\"Createc X validation tensor\")\n","    buy_x_validation_tensor = torch.from_numpy(buy_x_test)\n","    buy_x_validation_tensor = buy_x_validation_tensor.float()\n","except ValueError:\n","    print(\"Error: buy_x_train contains non-convertible values.\")\n","\n"]},{"cell_type":"markdown","id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508","metadata":{"id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508"},"source":["## Training and Tuning"]},{"cell_type":"markdown","id":"8rnk3fyyNbB4","metadata":{"id":"8rnk3fyyNbB4"},"source":["> Here we define the hyperparameters of the neural network and begin training the network with those parameters. As deep learning is an iterative process- with model degredation and improvements both contributing to overall progress- this section does not contain the history of experimental training and parameter tuning that moonShot has undergone."]},{"cell_type":"code","execution_count":null,"id":"ea6d6453-bed0-418b-a6f3-681f0c44efc9","metadata":{"id":"ea6d6453-bed0-418b-a6f3-681f0c44efc9"},"outputs":[],"source":["\n","input_size = len(buy_x_train_tensor[0])\n","hidden_size = 56\n","#56\n","output_size = 1\n","learning_rate = 0.00001\n","num_epochs = 100\n","\n","# Loss: 0.4661\n","# Loss with 2 layers: 0.4745\n"]},{"cell_type":"code","execution_count":null,"id":"6bece210-a088-40d4-8394-96388947100f","metadata":{"id":"6bece210-a088-40d4-8394-96388947100f"},"outputs":[],"source":["moonShot_buy = SimpleNN(input_size, hidden_size, output_size)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(moonShot_buy.parameters(), lr=learning_rate)\n","# optimizer = torch.optim.SGD(moonShot_buy.parameters(), lr=learning_rate, momentum=0.9)\n","# optimizer = torch.optim.Adagrad(moonShot_buy.parameters(), lr=learning_rate)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"50f7c5f7-070e-43f2-b017-74bab65881e1","metadata":{"id":"50f7c5f7-070e-43f2-b017-74bab65881e1"},"outputs":[],"source":["\n","def evaluate(model, x_val, y_val):\n","  \"\"\"\n","  This function evaluates the model performance on a validation set.\n","\n","  Args:\n","      model: The deep neural network model.\n","      x_val: Validation set input data.\n","      y_val: Validation set target labels.\n","\n","  Returns:\n","      val_loss: The validation loss (calculated using the criterion function).\n","      val_accuracy: The validation accuracy.\n","  \"\"\"\n","  with torch.no_grad():  # Deactivate gradient calculation for validation\n","    # Forward pass on validation set\n","    val_outputs = model(x_val)\n","    val_loss = criterion(val_outputs, y_val)\n","\n","    # Calculate accuracy\n","    predicted = (val_outputs > 0.5).float()  # Thresholding for binary classification\n","    val_accuracy = (predicted == y_val).sum() / len(y_val)\n","\n","  return val_loss.item(), val_accuracy.item()\n"]},{"cell_type":"code","execution_count":null,"id":"420dea90-74ee-40cd-a36f-c3ab34a05ec4","metadata":{"id":"420dea90-74ee-40cd-a36f-c3ab34a05ec4","outputId":"b4be6f80-a839-448c-ed80-3a47f0fd21bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [2/100], Train Loss: 0.6539, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [4/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [6/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [8/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [10/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [12/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [14/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [16/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [18/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [20/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [22/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [24/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6059\n","Epoch [26/100], Train Loss: 0.6538, Val Loss: 0.6631, Val Accuracy: 0.6080\n","Epoch [28/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [30/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [32/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [34/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6080\n","Epoch [36/100], Train Loss: 0.6538, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [38/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [40/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [42/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [44/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [46/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [48/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [50/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [52/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [54/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [56/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [58/100], Train Loss: 0.6537, Val Loss: 0.6630, Val Accuracy: 0.6101\n","Epoch [60/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [62/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [64/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [66/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [68/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [70/100], Train Loss: 0.6537, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [72/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [74/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [76/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [78/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [80/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6080\n","Epoch [82/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [84/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [86/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [88/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [90/100], Train Loss: 0.6536, Val Loss: 0.6629, Val Accuracy: 0.6101\n","Epoch [92/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [94/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [96/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [98/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6101\n","Epoch [100/100], Train Loss: 0.6536, Val Loss: 0.6628, Val Accuracy: 0.6122\n"]}],"source":["# Reshape target tensor to match output shape\n","buy_y_train_tensor = buy_y_train_tensor.view(-1, 1)\n","buy_y_validation_tensor = buy_y_validation_tensor.view(-1, 1)\n","\n","for epoch in range(num_epochs):\n","    moonShot_buy.train()\n","    optimizer.zero_grad()\n","\n","# The forward pass as defined in the neural network architecture\n","    outputs = moonShot_buy(buy_x_train_tensor)\n","    loss = criterion(outputs, buy_y_train_tensor)\n","\n","# Backward pass of the calculated loss\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","# Evaluate on validation set\n","    val_loss, val_accuracy = evaluate(moonShot_buy, buy_x_validation_tensor, buy_y_validation_tensor)\n","\n","# Print loss and validation metrics (optional)\n","    if (epoch + 1) % 2 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":null,"id":"c53146ea-d721-4bcb-a763-8cec88f6fae0","metadata":{"id":"c53146ea-d721-4bcb-a763-8cec88f6fae0"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
