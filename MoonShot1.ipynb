{"cells":[{"cell_type":"markdown","id":"1e866085-c160-4028-babd-0b2ca9b9fdba","metadata":{"id":"1e866085-c160-4028-babd-0b2ca9b9fdba"},"source":["# Project: MoonShot - AI-powered Trading Strategy\n","\n","This notebook outlines the development of MoonShot, an Artificial intelligence (AI) system designed to assess the increase in a single stock price against a wide array of technical indicators and use this information to develop a trading pattern. We aim to achieve the following objectives:\n","\n","---\n","\n","### Select effective technical indicators:\n","We will feed a number of technical indicators into the AI as additional training parameters, it will determine correlations between indicators and the increase in stock price.\n","From these technical indicator correlations we will select the most effective techincal indicators with the most correlation to a rise in stock price. \n","\n","---\n","\n","### Train a Neural Network:\n","We will train a multilayer neural network on historical market data and the corresponding stock price history as well as the technical indicators selected. This neural network will attempt to learn and potentially improve upon the initial strategy, potentially identifying new patterns or refining existing ones.\n","\n","We will consider the model a success if it is able to increase both the Win rate by 15% while maintaining the profit percentage.\n","\n","#### Throughout this notebook, we will document the development process, including:\n","\n","- Data acquisition and preparation\n","- Feature engineering and selection\n","- Building and training the neural network\n","- Evaluating the performance of MoonShot's strategy and the trained neural network\n","\n","---\n","\n","Disclaimer: This project is for educational purposes only and should not be used for real-world trading without proper risk management and regulatory compliance. The market is inherently risky, and any trading strategy, including those involving AI, is susceptible to losses."]},{"cell_type":"markdown","id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f","metadata":{"id":"356208bd-1d8b-46ca-90e8-79d877dfcf2f"},"source":["# Import Required Libraries"]},{"cell_type":"code","execution_count":6,"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc","metadata":{"id":"92b16fb9-647f-4fb0-b2ce-89031e8e79dc"},"outputs":[],"source":["# Required libraries\n","\n","# Import the generic libraries\n","import sys\n","import pytictoc\n","\n","#Import the neural network architecture\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","#Import financial data\n","import ta\n","import yfinance as yf\n","\n","# Import data science tools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pandas_datareader as pdr\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","\n","from tickers500 import Tickers500\n","from tickerTA import Ticker\n","from tickerTA import TechnicalAnalysis"]},{"cell_type":"markdown","id":"0098c760-272a-4988-9d91-c640825df298","metadata":{"id":"0098c760-272a-4988-9d91-c640825df298"},"source":["---\n","# Load and Preprocess Data"]},{"cell_type":"markdown","id":"faeae68d-c55e-43f5-b1b9-910034b34c61","metadata":{"id":"faeae68d-c55e-43f5-b1b9-910034b34c61"},"source":["## 1. Initialize our Tickers500 class, update all TickerData stored CSV files, import the DataFrame we are interested in from CSV"]},{"cell_type":"code","execution_count":7,"id":"a22ec22e","metadata":{},"outputs":[{"data":{"text/plain":["'EW'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tickers500 = Tickers500()\n","ticker = tickers500.get_random_ticker()\n","ticker\n"]},{"cell_type":"code","execution_count":8,"id":"a9d7a0e9","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Adj Close</th>\n","      <th>Volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-03-27</td>\n","      <td>1.270833</td>\n","      <td>1.385417</td>\n","      <td>1.270833</td>\n","      <td>1.375000</td>\n","      <td>1.375000</td>\n","      <td>11026800</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-03-28</td>\n","      <td>1.375000</td>\n","      <td>1.375000</td>\n","      <td>1.333333</td>\n","      <td>1.338542</td>\n","      <td>1.338542</td>\n","      <td>3232800</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-03-29</td>\n","      <td>1.333333</td>\n","      <td>1.343750</td>\n","      <td>1.317708</td>\n","      <td>1.333333</td>\n","      <td>1.333333</td>\n","      <td>1311600</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-03-30</td>\n","      <td>1.333333</td>\n","      <td>1.333333</td>\n","      <td>1.281250</td>\n","      <td>1.281250</td>\n","      <td>1.281250</td>\n","      <td>5650800</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-03-31</td>\n","      <td>1.244792</td>\n","      <td>1.244792</td>\n","      <td>1.130208</td>\n","      <td>1.130208</td>\n","      <td>1.130208</td>\n","      <td>23794800</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6053</th>\n","      <td>2024-04-18</td>\n","      <td>87.349998</td>\n","      <td>87.349998</td>\n","      <td>85.980003</td>\n","      <td>86.449997</td>\n","      <td>86.449997</td>\n","      <td>3122000</td>\n","    </tr>\n","    <tr>\n","      <th>6054</th>\n","      <td>2024-04-19</td>\n","      <td>87.199997</td>\n","      <td>87.199997</td>\n","      <td>85.379997</td>\n","      <td>85.940002</td>\n","      <td>85.940002</td>\n","      <td>3895700</td>\n","    </tr>\n","    <tr>\n","      <th>6055</th>\n","      <td>2024-04-22</td>\n","      <td>86.540001</td>\n","      <td>87.110001</td>\n","      <td>85.730003</td>\n","      <td>86.959999</td>\n","      <td>86.959999</td>\n","      <td>2408100</td>\n","    </tr>\n","    <tr>\n","      <th>6056</th>\n","      <td>2024-04-23</td>\n","      <td>87.400002</td>\n","      <td>87.930000</td>\n","      <td>86.760002</td>\n","      <td>87.750000</td>\n","      <td>87.750000</td>\n","      <td>2663600</td>\n","    </tr>\n","    <tr>\n","      <th>6057</th>\n","      <td>2024-04-23</td>\n","      <td>87.400002</td>\n","      <td>87.930000</td>\n","      <td>86.760002</td>\n","      <td>87.750000</td>\n","      <td>87.750000</td>\n","      <td>2562562</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6058 rows × 7 columns</p>\n","</div>"],"text/plain":["            Date       Open       High        Low      Close  Adj Close  \\\n","0     2000-03-27   1.270833   1.385417   1.270833   1.375000   1.375000   \n","1     2000-03-28   1.375000   1.375000   1.333333   1.338542   1.338542   \n","2     2000-03-29   1.333333   1.343750   1.317708   1.333333   1.333333   \n","3     2000-03-30   1.333333   1.333333   1.281250   1.281250   1.281250   \n","4     2000-03-31   1.244792   1.244792   1.130208   1.130208   1.130208   \n","...          ...        ...        ...        ...        ...        ...   \n","6053  2024-04-18  87.349998  87.349998  85.980003  86.449997  86.449997   \n","6054  2024-04-19  87.199997  87.199997  85.379997  85.940002  85.940002   \n","6055  2024-04-22  86.540001  87.110001  85.730003  86.959999  86.959999   \n","6056  2024-04-23  87.400002  87.930000  86.760002  87.750000  87.750000   \n","6057  2024-04-23  87.400002  87.930000  86.760002  87.750000  87.750000   \n","\n","        Volume  \n","0     11026800  \n","1      3232800  \n","2      1311600  \n","3      5650800  \n","4     23794800  \n","...        ...  \n","6053   3122000  \n","6054   3895700  \n","6055   2408100  \n","6056   2663600  \n","6057   2562562  \n","\n","[6058 rows x 7 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# start_date = '2020-01-01'\n","ticker_df = tickers500.load_ticker_data_to_df(ticker)\n","# ticker_df = tickers500.load_ticker_data_to_df(ticker, start_date)\n","ticker_df"]},{"cell_type":"markdown","id":"cd7c54f0","metadata":{},"source":["## 2. Send that DataFrame through TechnicalAnalysis class to have the technical indicators added on to a DataFrame."]},{"cell_type":"code","execution_count":9,"id":"96c1880b","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Adj Close</th>\n","      <th>Volume</th>\n","      <th>Z Score Adj Close</th>\n","      <th>MACD</th>\n","      <th>MACD Signal</th>\n","      <th>...</th>\n","      <th>RSI</th>\n","      <th>TSI</th>\n","      <th>Awesome Oscillator</th>\n","      <th>Ultimate Oscillator</th>\n","      <th>Stoch</th>\n","      <th>Stoch Signal</th>\n","      <th>Williams %R</th>\n","      <th>KAMA</th>\n","      <th>PPO</th>\n","      <th>ROC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-03-27</td>\n","      <td>1.270833</td>\n","      <td>1.385417</td>\n","      <td>1.270833</td>\n","      <td>1.375000</td>\n","      <td>1.375000</td>\n","      <td>11026800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-03-28</td>\n","      <td>1.375000</td>\n","      <td>1.375000</td>\n","      <td>1.333333</td>\n","      <td>1.338542</td>\n","      <td>1.338542</td>\n","      <td>3232800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-03-29</td>\n","      <td>1.333333</td>\n","      <td>1.343750</td>\n","      <td>1.317708</td>\n","      <td>1.333333</td>\n","      <td>1.333333</td>\n","      <td>1311600</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-03-30</td>\n","      <td>1.333333</td>\n","      <td>1.333333</td>\n","      <td>1.281250</td>\n","      <td>1.281250</td>\n","      <td>1.281250</td>\n","      <td>5650800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-03-31</td>\n","      <td>1.244792</td>\n","      <td>1.244792</td>\n","      <td>1.130208</td>\n","      <td>1.130208</td>\n","      <td>1.130208</td>\n","      <td>23794800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6053</th>\n","      <td>2024-04-18</td>\n","      <td>87.349998</td>\n","      <td>87.349998</td>\n","      <td>85.980003</td>\n","      <td>86.449997</td>\n","      <td>86.449997</td>\n","      <td>3122000</td>\n","      <td>-2.278788</td>\n","      <td>-0.983743</td>\n","      <td>0.464616</td>\n","      <td>...</td>\n","      <td>19.924083</td>\n","      <td>-6.340158</td>\n","      <td>-2.731000</td>\n","      <td>38.448934</td>\n","      <td>4.820447</td>\n","      <td>13.138098</td>\n","      <td>-95.179553</td>\n","      <td>89.860998</td>\n","      <td>-0.572482</td>\n","      <td>-8.334218</td>\n","    </tr>\n","    <tr>\n","      <th>6054</th>\n","      <td>2024-04-19</td>\n","      <td>87.199997</td>\n","      <td>87.199997</td>\n","      <td>85.379997</td>\n","      <td>85.940002</td>\n","      <td>85.940002</td>\n","      <td>3895700</td>\n","      <td>-2.074598</td>\n","      <td>-1.025598</td>\n","      <td>0.208217</td>\n","      <td>...</td>\n","      <td>18.339316</td>\n","      <td>-10.247711</td>\n","      <td>-3.536500</td>\n","      <td>38.387253</td>\n","      <td>5.779206</td>\n","      <td>6.256418</td>\n","      <td>-94.220794</td>\n","      <td>88.970680</td>\n","      <td>-0.904894</td>\n","      <td>-7.611260</td>\n","    </tr>\n","    <tr>\n","      <th>6055</th>\n","      <td>2024-04-22</td>\n","      <td>86.540001</td>\n","      <td>87.110001</td>\n","      <td>85.730003</td>\n","      <td>86.959999</td>\n","      <td>86.959999</td>\n","      <td>2408100</td>\n","      <td>-1.505336</td>\n","      <td>-0.934872</td>\n","      <td>-0.025502</td>\n","      <td>...</td>\n","      <td>31.429321</td>\n","      <td>-12.218100</td>\n","      <td>-4.320471</td>\n","      <td>41.849286</td>\n","      <td>16.305484</td>\n","      <td>8.968379</td>\n","      <td>-83.694516</td>\n","      <td>88.692621</td>\n","      <td>-1.066142</td>\n","      <td>-4.649121</td>\n","    </tr>\n","    <tr>\n","      <th>6056</th>\n","      <td>2024-04-23</td>\n","      <td>87.400002</td>\n","      <td>87.930000</td>\n","      <td>86.760002</td>\n","      <td>87.750000</td>\n","      <td>87.750000</td>\n","      <td>2663600</td>\n","      <td>-1.114287</td>\n","      <td>-0.778351</td>\n","      <td>-0.220089</td>\n","      <td>...</td>\n","      <td>40.320548</td>\n","      <td>-12.889955</td>\n","      <td>-4.702441</td>\n","      <td>44.519442</td>\n","      <td>26.362640</td>\n","      <td>16.149110</td>\n","      <td>-73.637360</td>\n","      <td>88.567023</td>\n","      <td>-1.110528</td>\n","      <td>-5.339803</td>\n","    </tr>\n","    <tr>\n","      <th>6057</th>\n","      <td>2024-04-23</td>\n","      <td>87.400002</td>\n","      <td>87.930000</td>\n","      <td>86.760002</td>\n","      <td>87.750000</td>\n","      <td>87.750000</td>\n","      <td>2562562</td>\n","      <td>-1.007620</td>\n","      <td>-0.637438</td>\n","      <td>-0.379449</td>\n","      <td>...</td>\n","      <td>40.320548</td>\n","      <td>-13.432065</td>\n","      <td>-4.863911</td>\n","      <td>53.030584</td>\n","      <td>30.659789</td>\n","      <td>24.442638</td>\n","      <td>-69.340211</td>\n","      <td>88.486537</td>\n","      <td>-1.133059</td>\n","      <td>-4.970756</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6058 rows × 21 columns</p>\n","</div>"],"text/plain":["            Date       Open       High        Low      Close  Adj Close  \\\n","0     2000-03-27   1.270833   1.385417   1.270833   1.375000   1.375000   \n","1     2000-03-28   1.375000   1.375000   1.333333   1.338542   1.338542   \n","2     2000-03-29   1.333333   1.343750   1.317708   1.333333   1.333333   \n","3     2000-03-30   1.333333   1.333333   1.281250   1.281250   1.281250   \n","4     2000-03-31   1.244792   1.244792   1.130208   1.130208   1.130208   \n","...          ...        ...        ...        ...        ...        ...   \n","6053  2024-04-18  87.349998  87.349998  85.980003  86.449997  86.449997   \n","6054  2024-04-19  87.199997  87.199997  85.379997  85.940002  85.940002   \n","6055  2024-04-22  86.540001  87.110001  85.730003  86.959999  86.959999   \n","6056  2024-04-23  87.400002  87.930000  86.760002  87.750000  87.750000   \n","6057  2024-04-23  87.400002  87.930000  86.760002  87.750000  87.750000   \n","\n","        Volume  Z Score Adj Close      MACD  MACD Signal  ...        RSI  \\\n","0     11026800                NaN       NaN          NaN  ...        NaN   \n","1      3232800                NaN       NaN          NaN  ...        NaN   \n","2      1311600                NaN       NaN          NaN  ...        NaN   \n","3      5650800                NaN       NaN          NaN  ...        NaN   \n","4     23794800                NaN       NaN          NaN  ...        NaN   \n","...        ...                ...       ...          ...  ...        ...   \n","6053   3122000          -2.278788 -0.983743     0.464616  ...  19.924083   \n","6054   3895700          -2.074598 -1.025598     0.208217  ...  18.339316   \n","6055   2408100          -1.505336 -0.934872    -0.025502  ...  31.429321   \n","6056   2663600          -1.114287 -0.778351    -0.220089  ...  40.320548   \n","6057   2562562          -1.007620 -0.637438    -0.379449  ...  40.320548   \n","\n","            TSI  Awesome Oscillator  Ultimate Oscillator      Stoch  \\\n","0           NaN                 NaN                  NaN        NaN   \n","1           NaN                 NaN                  NaN        NaN   \n","2           NaN                 NaN                  NaN        NaN   \n","3           NaN                 NaN                  NaN        NaN   \n","4           NaN                 NaN                  NaN        NaN   \n","...         ...                 ...                  ...        ...   \n","6053  -6.340158           -2.731000            38.448934   4.820447   \n","6054 -10.247711           -3.536500            38.387253   5.779206   \n","6055 -12.218100           -4.320471            41.849286  16.305484   \n","6056 -12.889955           -4.702441            44.519442  26.362640   \n","6057 -13.432065           -4.863911            53.030584  30.659789   \n","\n","      Stoch Signal  Williams %R       KAMA       PPO       ROC  \n","0              NaN          NaN        NaN       NaN       NaN  \n","1              NaN          NaN        NaN       NaN       NaN  \n","2              NaN          NaN        NaN       NaN       NaN  \n","3              NaN          NaN        NaN       NaN       NaN  \n","4              NaN          NaN        NaN       NaN       NaN  \n","...            ...          ...        ...       ...       ...  \n","6053     13.138098   -95.179553  89.860998 -0.572482 -8.334218  \n","6054      6.256418   -94.220794  88.970680 -0.904894 -7.611260  \n","6055      8.968379   -83.694516  88.692621 -1.066142 -4.649121  \n","6056     16.149110   -73.637360  88.567023 -1.110528 -5.339803  \n","6057     24.442638   -69.340211  88.486537 -1.133059 -4.970756  \n","\n","[6058 rows x 21 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["technical_analysis = TechnicalAnalysis(ticker, ticker_df)\n","technical_analysis.df_ta"]},{"cell_type":"code","execution_count":10,"id":"fe4f3fce","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Adj Close</th>\n","      <th>Z Score Adj Close</th>\n","      <th>MACD</th>\n","      <th>MACD Signal</th>\n","      <th>MACD Histogram</th>\n","      <th>RSI</th>\n","      <th>TSI</th>\n","      <th>Awesome Oscillator</th>\n","      <th>Ultimate Oscillator</th>\n","      <th>Stoch</th>\n","      <th>Stoch Signal</th>\n","      <th>Williams %R</th>\n","      <th>KAMA</th>\n","      <th>PPO</th>\n","      <th>ROC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.375000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.338542</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.333333</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.281250</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.130208</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6053</th>\n","      <td>86.449997</td>\n","      <td>-2.278788</td>\n","      <td>-0.983743</td>\n","      <td>0.464616</td>\n","      <td>-1.448359</td>\n","      <td>19.924083</td>\n","      <td>-6.340158</td>\n","      <td>-2.731000</td>\n","      <td>38.448934</td>\n","      <td>4.820447</td>\n","      <td>13.138098</td>\n","      <td>-95.179553</td>\n","      <td>89.860998</td>\n","      <td>-0.572482</td>\n","      <td>-8.334218</td>\n","    </tr>\n","    <tr>\n","      <th>6054</th>\n","      <td>85.940002</td>\n","      <td>-2.074598</td>\n","      <td>-1.025598</td>\n","      <td>0.208217</td>\n","      <td>-1.233814</td>\n","      <td>18.339316</td>\n","      <td>-10.247711</td>\n","      <td>-3.536500</td>\n","      <td>38.387253</td>\n","      <td>5.779206</td>\n","      <td>6.256418</td>\n","      <td>-94.220794</td>\n","      <td>88.970680</td>\n","      <td>-0.904894</td>\n","      <td>-7.611260</td>\n","    </tr>\n","    <tr>\n","      <th>6055</th>\n","      <td>86.959999</td>\n","      <td>-1.505336</td>\n","      <td>-0.934872</td>\n","      <td>-0.025502</td>\n","      <td>-0.909371</td>\n","      <td>31.429321</td>\n","      <td>-12.218100</td>\n","      <td>-4.320471</td>\n","      <td>41.849286</td>\n","      <td>16.305484</td>\n","      <td>8.968379</td>\n","      <td>-83.694516</td>\n","      <td>88.692621</td>\n","      <td>-1.066142</td>\n","      <td>-4.649121</td>\n","    </tr>\n","    <tr>\n","      <th>6056</th>\n","      <td>87.750000</td>\n","      <td>-1.114287</td>\n","      <td>-0.778351</td>\n","      <td>-0.220089</td>\n","      <td>-0.558262</td>\n","      <td>40.320548</td>\n","      <td>-12.889955</td>\n","      <td>-4.702441</td>\n","      <td>44.519442</td>\n","      <td>26.362640</td>\n","      <td>16.149110</td>\n","      <td>-73.637360</td>\n","      <td>88.567023</td>\n","      <td>-1.110528</td>\n","      <td>-5.339803</td>\n","    </tr>\n","    <tr>\n","      <th>6057</th>\n","      <td>87.750000</td>\n","      <td>-1.007620</td>\n","      <td>-0.637438</td>\n","      <td>-0.379449</td>\n","      <td>-0.257989</td>\n","      <td>40.320548</td>\n","      <td>-13.432065</td>\n","      <td>-4.863911</td>\n","      <td>53.030584</td>\n","      <td>30.659789</td>\n","      <td>24.442638</td>\n","      <td>-69.340211</td>\n","      <td>88.486537</td>\n","      <td>-1.133059</td>\n","      <td>-4.970756</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6058 rows × 15 columns</p>\n","</div>"],"text/plain":["      Adj Close  Z Score Adj Close      MACD  MACD Signal  MACD Histogram  \\\n","0      1.375000                NaN       NaN          NaN             NaN   \n","1      1.338542                NaN       NaN          NaN             NaN   \n","2      1.333333                NaN       NaN          NaN             NaN   \n","3      1.281250                NaN       NaN          NaN             NaN   \n","4      1.130208                NaN       NaN          NaN             NaN   \n","...         ...                ...       ...          ...             ...   \n","6053  86.449997          -2.278788 -0.983743     0.464616       -1.448359   \n","6054  85.940002          -2.074598 -1.025598     0.208217       -1.233814   \n","6055  86.959999          -1.505336 -0.934872    -0.025502       -0.909371   \n","6056  87.750000          -1.114287 -0.778351    -0.220089       -0.558262   \n","6057  87.750000          -1.007620 -0.637438    -0.379449       -0.257989   \n","\n","            RSI        TSI  Awesome Oscillator  Ultimate Oscillator  \\\n","0           NaN        NaN                 NaN                  NaN   \n","1           NaN        NaN                 NaN                  NaN   \n","2           NaN        NaN                 NaN                  NaN   \n","3           NaN        NaN                 NaN                  NaN   \n","4           NaN        NaN                 NaN                  NaN   \n","...         ...        ...                 ...                  ...   \n","6053  19.924083  -6.340158           -2.731000            38.448934   \n","6054  18.339316 -10.247711           -3.536500            38.387253   \n","6055  31.429321 -12.218100           -4.320471            41.849286   \n","6056  40.320548 -12.889955           -4.702441            44.519442   \n","6057  40.320548 -13.432065           -4.863911            53.030584   \n","\n","          Stoch  Stoch Signal  Williams %R       KAMA       PPO       ROC  \n","0           NaN           NaN          NaN        NaN       NaN       NaN  \n","1           NaN           NaN          NaN        NaN       NaN       NaN  \n","2           NaN           NaN          NaN        NaN       NaN       NaN  \n","3           NaN           NaN          NaN        NaN       NaN       NaN  \n","4           NaN           NaN          NaN        NaN       NaN       NaN  \n","...         ...           ...          ...        ...       ...       ...  \n","6053   4.820447     13.138098   -95.179553  89.860998 -0.572482 -8.334218  \n","6054   5.779206      6.256418   -94.220794  88.970680 -0.904894 -7.611260  \n","6055  16.305484      8.968379   -83.694516  88.692621 -1.066142 -4.649121  \n","6056  26.362640     16.149110   -73.637360  88.567023 -1.110528 -5.339803  \n","6057  30.659789     24.442638   -69.340211  88.486537 -1.133059 -4.970756  \n","\n","[6058 rows x 15 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["technical_analysis.df_ta.drop(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n","technical_analysis.df_ta"]},{"cell_type":"code","execution_count":18,"id":"4209a7fc","metadata":{},"outputs":[],"source":["technical_analysis.df_ta.dropna(inplace=True)"]},{"cell_type":"markdown","id":"112cebde-ab83-4697-b412-0748cc885f54","metadata":{"id":"112cebde-ab83-4697-b412-0748cc885f54"},"source":["### preprocess the main dataset\n","\n","* Drop data not required by any subsets\n","* Split the data into the X and Y sets\n","* Handle missing data\n","* scaling, normalization, and correlations"]},{"cell_type":"code","execution_count":19,"id":"1be8c379","metadata":{},"outputs":[],"source":["# Two new dataframes are created, one contains all of the input features of the dataset (x), the other contains the target values (y)\n","x = technical_analysis.df_ta.drop(columns=['Adj Close'])\n","price_diff = technical_analysis.df_ta['Adj Close'].diff()\n","y = price_diff.apply(lambda x: 1 if x > 0 else 0)\n","\n","\n","# The data is scaled\n","scaler = StandardScaler()\n","# scaler.fit(x)\n","scaler.fit_transform(x)\n","# x_train = sc.fit_transform(x_train)\n","x_scaled = scaler.transform(x)\n","\n","# The data is split into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=0)\n","\n","# The data is converted to PyTorch tensors\n","x_train = torch.FloatTensor(x_train).to('cuda')\n","x_test = torch.FloatTensor(x_test).to('cuda')\n","y_train = torch.FloatTensor(y_train.to_numpy().copy()).to('cuda')\n","y_test = torch.FloatTensor(y_test.to_numpy().copy()).to('cuda')\n"]},{"cell_type":"code","execution_count":20,"id":"ed885db6","metadata":{},"outputs":[],"source":["\n","# The neural network is defined\n","class ANN(nn.Module):\n","    # def __init__(self, input_features=9, hidden1=20, hidden2=20, output_features=1):\n","    def __init__(self, input_features, hidden1, hidden2, output_features):\n","        super().__init__()\n","        self.f_connected1 = nn.Linear(input_features, hidden1)\n","        self.f_connected2 = nn.Linear(hidden1, hidden2)\n","        self.out = nn.Linear(hidden2, output_features)\n","    def forward(self, x):\n","        x = torch.relu(self.f_connected1(x))\n","        x = torch.relu(self.f_connected2(x))\n","        x = self.out(x)\n","        return x\n","    \n"]},{"cell_type":"code","execution_count":21,"id":"a26a2f28","metadata":{},"outputs":[],"source":["def evaluate_model(model, x_test, y_test):\n","    with torch.no_grad():\n","        y_val = model(x_test)\n","        loss = loss_function(y_val, y_test)\n","\n","        # calculate accuracy\n","        y_val = model(x_test)\n","        predicted = torch.argmax(y_val, 1)\n","        correct = (predicted == y_test).sum().item()\n","        accuracy = correct / y_test.shape[0]\n","    return accuracy, loss.item()"]},{"cell_type":"code","execution_count":22,"id":"fa67388e","metadata":{},"outputs":[],"source":["# The model is instantiated\n","input_size = len(x_train[0])\n","hidden_size1 = 56\n","hidden_size2 = 56\n","output_size = 1\n","torch.manual_seed(20)\n","model = ANN(input_size, hidden_size1, hidden_size2, output_size)\n","model.to('cuda')\n","\n","# The loss function and optimizer are defined\n","loss_function = nn.MSELoss()\n","# criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":23,"id":"8c169f5c","metadata":{},"outputs":[],"source":["\n","# The model is trained\n","epochs = 10000\n","final_losses = []\n"]},{"cell_type":"code","execution_count":26,"id":"faa080ef","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [2/10000], Train Loss: 0.0353, Val Loss: 0.2577, Val Accuracy: 0.4813\n","Epoch [12/10000], Train Loss: 0.0347, Val Loss: 0.2559, Val Accuracy: 0.4813\n","Epoch [22/10000], Train Loss: 0.0362, Val Loss: 0.2579, Val Accuracy: 0.4813\n","Epoch [32/10000], Train Loss: 0.0345, Val Loss: 0.2572, Val Accuracy: 0.4813\n","Epoch [42/10000], Train Loss: 0.0372, Val Loss: 0.2614, Val Accuracy: 0.4813\n","Epoch [52/10000], Train Loss: 0.0400, Val Loss: 0.2614, Val Accuracy: 0.4813\n","Epoch [62/10000], Train Loss: 0.0365, Val Loss: 0.2541, Val Accuracy: 0.4813\n","Epoch [72/10000], Train Loss: 0.0349, Val Loss: 0.2574, Val Accuracy: 0.4813\n","Epoch [82/10000], Train Loss: 0.0360, Val Loss: 0.2571, Val Accuracy: 0.4813\n","Epoch [92/10000], Train Loss: 0.0342, Val Loss: 0.2567, Val Accuracy: 0.4813\n","Epoch [102/10000], Train Loss: 0.0359, Val Loss: 0.2575, Val Accuracy: 0.4813\n","Epoch [112/10000], Train Loss: 0.0458, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [122/10000], Train Loss: 0.0369, Val Loss: 0.2579, Val Accuracy: 0.4813\n","Epoch [132/10000], Train Loss: 0.0351, Val Loss: 0.2550, Val Accuracy: 0.4813\n","Epoch [142/10000], Train Loss: 0.0354, Val Loss: 0.2569, Val Accuracy: 0.4813\n","Epoch [152/10000], Train Loss: 0.0358, Val Loss: 0.2561, Val Accuracy: 0.4813\n","Epoch [162/10000], Train Loss: 0.0369, Val Loss: 0.2575, Val Accuracy: 0.4813\n","Epoch [172/10000], Train Loss: 0.0368, Val Loss: 0.2586, Val Accuracy: 0.4813\n","Epoch [182/10000], Train Loss: 0.0381, Val Loss: 0.2582, Val Accuracy: 0.4813\n","Epoch [192/10000], Train Loss: 0.0369, Val Loss: 0.2573, Val Accuracy: 0.4813\n","Epoch [202/10000], Train Loss: 0.0352, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [212/10000], Train Loss: 0.0413, Val Loss: 0.2617, Val Accuracy: 0.4813\n","Epoch [222/10000], Train Loss: 0.0351, Val Loss: 0.2591, Val Accuracy: 0.4813\n","Epoch [232/10000], Train Loss: 0.0355, Val Loss: 0.2546, Val Accuracy: 0.4813\n","Epoch [242/10000], Train Loss: 0.0356, Val Loss: 0.2573, Val Accuracy: 0.4813\n","Epoch [252/10000], Train Loss: 0.0345, Val Loss: 0.2568, Val Accuracy: 0.4813\n","Epoch [262/10000], Train Loss: 0.0351, Val Loss: 0.2577, Val Accuracy: 0.4813\n","Epoch [272/10000], Train Loss: 0.0438, Val Loss: 0.2624, Val Accuracy: 0.4813\n","Epoch [282/10000], Train Loss: 0.0353, Val Loss: 0.2589, Val Accuracy: 0.4813\n","Epoch [292/10000], Train Loss: 0.0369, Val Loss: 0.2614, Val Accuracy: 0.4813\n","Epoch [302/10000], Train Loss: 0.0339, Val Loss: 0.2585, Val Accuracy: 0.4813\n","Epoch [312/10000], Train Loss: 0.0378, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [322/10000], Train Loss: 0.0360, Val Loss: 0.2565, Val Accuracy: 0.4813\n","Epoch [332/10000], Train Loss: 0.0343, Val Loss: 0.2553, Val Accuracy: 0.4813\n","Epoch [342/10000], Train Loss: 0.0346, Val Loss: 0.2588, Val Accuracy: 0.4813\n","Epoch [352/10000], Train Loss: 0.0342, Val Loss: 0.2572, Val Accuracy: 0.4813\n","Epoch [362/10000], Train Loss: 0.0367, Val Loss: 0.2583, Val Accuracy: 0.4813\n","Epoch [372/10000], Train Loss: 0.0345, Val Loss: 0.2573, Val Accuracy: 0.4813\n","Epoch [382/10000], Train Loss: 0.0340, Val Loss: 0.2575, Val Accuracy: 0.4813\n","Epoch [392/10000], Train Loss: 0.0340, Val Loss: 0.2587, Val Accuracy: 0.4813\n","Epoch [402/10000], Train Loss: 0.0630, Val Loss: 0.2810, Val Accuracy: 0.4813\n","Epoch [412/10000], Train Loss: 0.0484, Val Loss: 0.2714, Val Accuracy: 0.4813\n","Epoch [422/10000], Train Loss: 0.0422, Val Loss: 0.2599, Val Accuracy: 0.4813\n","Epoch [432/10000], Train Loss: 0.0349, Val Loss: 0.2544, Val Accuracy: 0.4813\n","Epoch [442/10000], Train Loss: 0.0350, Val Loss: 0.2546, Val Accuracy: 0.4813\n","Epoch [452/10000], Train Loss: 0.0343, Val Loss: 0.2554, Val Accuracy: 0.4813\n","Epoch [462/10000], Train Loss: 0.0340, Val Loss: 0.2565, Val Accuracy: 0.4813\n","Epoch [472/10000], Train Loss: 0.0350, Val Loss: 0.2573, Val Accuracy: 0.4813\n","Epoch [482/10000], Train Loss: 0.0376, Val Loss: 0.2588, Val Accuracy: 0.4813\n","Epoch [492/10000], Train Loss: 0.0350, Val Loss: 0.2582, Val Accuracy: 0.4813\n","Epoch [502/10000], Train Loss: 0.0360, Val Loss: 0.2579, Val Accuracy: 0.4813\n","Epoch [512/10000], Train Loss: 0.0457, Val Loss: 0.2628, Val Accuracy: 0.4813\n","Epoch [522/10000], Train Loss: 0.0387, Val Loss: 0.2629, Val Accuracy: 0.4813\n","Epoch [532/10000], Train Loss: 0.0340, Val Loss: 0.2569, Val Accuracy: 0.4813\n","Epoch [542/10000], Train Loss: 0.0374, Val Loss: 0.2588, Val Accuracy: 0.4813\n","Epoch [552/10000], Train Loss: 0.0364, Val Loss: 0.2587, Val Accuracy: 0.4813\n","Epoch [562/10000], Train Loss: 0.0345, Val Loss: 0.2593, Val Accuracy: 0.4813\n","Epoch [572/10000], Train Loss: 0.0415, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [582/10000], Train Loss: 0.0376, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [592/10000], Train Loss: 0.0409, Val Loss: 0.2601, Val Accuracy: 0.4813\n","Epoch [602/10000], Train Loss: 0.0397, Val Loss: 0.2652, Val Accuracy: 0.4813\n","Epoch [612/10000], Train Loss: 0.0385, Val Loss: 0.2571, Val Accuracy: 0.4813\n","Epoch [622/10000], Train Loss: 0.0371, Val Loss: 0.2620, Val Accuracy: 0.4813\n","Epoch [632/10000], Train Loss: 0.0346, Val Loss: 0.2572, Val Accuracy: 0.4813\n","Epoch [642/10000], Train Loss: 0.0356, Val Loss: 0.2578, Val Accuracy: 0.4813\n","Epoch [652/10000], Train Loss: 0.0352, Val Loss: 0.2576, Val Accuracy: 0.4813\n","Epoch [662/10000], Train Loss: 0.0402, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [672/10000], Train Loss: 0.0338, Val Loss: 0.2565, Val Accuracy: 0.4813\n","Epoch [682/10000], Train Loss: 0.0389, Val Loss: 0.2631, Val Accuracy: 0.4813\n","Epoch [692/10000], Train Loss: 0.0347, Val Loss: 0.2566, Val Accuracy: 0.4813\n","Epoch [702/10000], Train Loss: 0.0400, Val Loss: 0.2607, Val Accuracy: 0.4813\n","Epoch [712/10000], Train Loss: 0.0359, Val Loss: 0.2569, Val Accuracy: 0.4813\n","Epoch [722/10000], Train Loss: 0.0405, Val Loss: 0.2655, Val Accuracy: 0.4813\n","Epoch [732/10000], Train Loss: 0.0354, Val Loss: 0.2577, Val Accuracy: 0.4813\n","Epoch [742/10000], Train Loss: 0.0368, Val Loss: 0.2585, Val Accuracy: 0.4813\n","Epoch [752/10000], Train Loss: 0.0344, Val Loss: 0.2604, Val Accuracy: 0.4813\n","Epoch [762/10000], Train Loss: 0.0450, Val Loss: 0.2724, Val Accuracy: 0.4813\n","Epoch [772/10000], Train Loss: 0.0357, Val Loss: 0.2577, Val Accuracy: 0.4813\n","Epoch [782/10000], Train Loss: 0.0337, Val Loss: 0.2572, Val Accuracy: 0.4813\n","Epoch [792/10000], Train Loss: 0.0366, Val Loss: 0.2613, Val Accuracy: 0.4813\n","Epoch [802/10000], Train Loss: 0.0337, Val Loss: 0.2584, Val Accuracy: 0.4813\n","Epoch [812/10000], Train Loss: 0.0445, Val Loss: 0.2660, Val Accuracy: 0.4813\n","Epoch [822/10000], Train Loss: 0.0357, Val Loss: 0.2617, Val Accuracy: 0.4813\n","Epoch [832/10000], Train Loss: 0.0348, Val Loss: 0.2584, Val Accuracy: 0.4813\n","Epoch [842/10000], Train Loss: 0.0365, Val Loss: 0.2600, Val Accuracy: 0.4813\n","Epoch [852/10000], Train Loss: 0.0343, Val Loss: 0.2593, Val Accuracy: 0.4813\n","Epoch [862/10000], Train Loss: 0.0417, Val Loss: 0.2672, Val Accuracy: 0.4813\n","Epoch [872/10000], Train Loss: 0.0337, Val Loss: 0.2584, Val Accuracy: 0.4813\n","Epoch [882/10000], Train Loss: 0.0405, Val Loss: 0.2620, Val Accuracy: 0.4813\n","Epoch [892/10000], Train Loss: 0.0346, Val Loss: 0.2584, Val Accuracy: 0.4813\n","Epoch [902/10000], Train Loss: 0.0346, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [912/10000], Train Loss: 0.0534, Val Loss: 0.2786, Val Accuracy: 0.4813\n","Epoch [922/10000], Train Loss: 0.0455, Val Loss: 0.2640, Val Accuracy: 0.4813\n","Epoch [932/10000], Train Loss: 0.0386, Val Loss: 0.2629, Val Accuracy: 0.4813\n","Epoch [942/10000], Train Loss: 0.0365, Val Loss: 0.2591, Val Accuracy: 0.4813\n","Epoch [952/10000], Train Loss: 0.0338, Val Loss: 0.2575, Val Accuracy: 0.4813\n","Epoch [962/10000], Train Loss: 0.0364, Val Loss: 0.2636, Val Accuracy: 0.4813\n","Epoch [972/10000], Train Loss: 0.0393, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [982/10000], Train Loss: 0.0377, Val Loss: 0.2611, Val Accuracy: 0.4813\n","Epoch [992/10000], Train Loss: 0.0340, Val Loss: 0.2582, Val Accuracy: 0.4813\n","Epoch [1002/10000], Train Loss: 0.0342, Val Loss: 0.2599, Val Accuracy: 0.4813\n","Epoch [1012/10000], Train Loss: 0.0526, Val Loss: 0.2795, Val Accuracy: 0.4813\n","Epoch [1022/10000], Train Loss: 0.0437, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [1032/10000], Train Loss: 0.0382, Val Loss: 0.2639, Val Accuracy: 0.4813\n","Epoch [1042/10000], Train Loss: 0.0354, Val Loss: 0.2582, Val Accuracy: 0.4813\n","Epoch [1052/10000], Train Loss: 0.0341, Val Loss: 0.2573, Val Accuracy: 0.4813\n","Epoch [1062/10000], Train Loss: 0.0348, Val Loss: 0.2621, Val Accuracy: 0.4813\n","Epoch [1072/10000], Train Loss: 0.0447, Val Loss: 0.2736, Val Accuracy: 0.4813\n","Epoch [1082/10000], Train Loss: 0.0364, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [1092/10000], Train Loss: 0.0351, Val Loss: 0.2588, Val Accuracy: 0.4813\n","Epoch [1102/10000], Train Loss: 0.0335, Val Loss: 0.2592, Val Accuracy: 0.4813\n","Epoch [1112/10000], Train Loss: 0.0433, Val Loss: 0.2707, Val Accuracy: 0.4813\n","Epoch [1122/10000], Train Loss: 0.0370, Val Loss: 0.2585, Val Accuracy: 0.4813\n","Epoch [1132/10000], Train Loss: 0.0373, Val Loss: 0.2608, Val Accuracy: 0.4813\n","Epoch [1142/10000], Train Loss: 0.0350, Val Loss: 0.2585, Val Accuracy: 0.4813\n","Epoch [1152/10000], Train Loss: 0.0341, Val Loss: 0.2575, Val Accuracy: 0.4813\n","Epoch [1162/10000], Train Loss: 0.0341, Val Loss: 0.2612, Val Accuracy: 0.4813\n","Epoch [1172/10000], Train Loss: 0.0435, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [1182/10000], Train Loss: 0.0341, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [1192/10000], Train Loss: 0.0363, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [1202/10000], Train Loss: 0.0343, Val Loss: 0.2600, Val Accuracy: 0.4813\n","Epoch [1212/10000], Train Loss: 0.0409, Val Loss: 0.2691, Val Accuracy: 0.4813\n","Epoch [1222/10000], Train Loss: 0.0337, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [1232/10000], Train Loss: 0.0397, Val Loss: 0.2629, Val Accuracy: 0.4813\n","Epoch [1242/10000], Train Loss: 0.0342, Val Loss: 0.2610, Val Accuracy: 0.4813\n","Epoch [1252/10000], Train Loss: 0.0401, Val Loss: 0.2690, Val Accuracy: 0.4813\n","Epoch [1262/10000], Train Loss: 0.0343, Val Loss: 0.2605, Val Accuracy: 0.4813\n","Epoch [1272/10000], Train Loss: 0.0384, Val Loss: 0.2610, Val Accuracy: 0.4813\n","Epoch [1282/10000], Train Loss: 0.0353, Val Loss: 0.2616, Val Accuracy: 0.4813\n","Epoch [1292/10000], Train Loss: 0.0375, Val Loss: 0.2623, Val Accuracy: 0.4813\n","Epoch [1302/10000], Train Loss: 0.0339, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [1312/10000], Train Loss: 0.0401, Val Loss: 0.2642, Val Accuracy: 0.4813\n","Epoch [1322/10000], Train Loss: 0.0337, Val Loss: 0.2586, Val Accuracy: 0.4813\n","Epoch [1332/10000], Train Loss: 0.0346, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [1342/10000], Train Loss: 0.0348, Val Loss: 0.2607, Val Accuracy: 0.4813\n","Epoch [1352/10000], Train Loss: 0.0338, Val Loss: 0.2581, Val Accuracy: 0.4813\n","Epoch [1362/10000], Train Loss: 0.0340, Val Loss: 0.2592, Val Accuracy: 0.4813\n","Epoch [1372/10000], Train Loss: 0.0336, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [1382/10000], Train Loss: 0.0400, Val Loss: 0.2680, Val Accuracy: 0.4813\n","Epoch [1392/10000], Train Loss: 0.0342, Val Loss: 0.2596, Val Accuracy: 0.4813\n","Epoch [1402/10000], Train Loss: 0.0346, Val Loss: 0.2590, Val Accuracy: 0.4813\n","Epoch [1412/10000], Train Loss: 0.0340, Val Loss: 0.2592, Val Accuracy: 0.4813\n","Epoch [1422/10000], Train Loss: 0.0344, Val Loss: 0.2588, Val Accuracy: 0.4813\n","Epoch [1432/10000], Train Loss: 0.0335, Val Loss: 0.2617, Val Accuracy: 0.4813\n","Epoch [1442/10000], Train Loss: 0.0415, Val Loss: 0.2703, Val Accuracy: 0.4813\n","Epoch [1452/10000], Train Loss: 0.0333, Val Loss: 0.2605, Val Accuracy: 0.4813\n","Epoch [1462/10000], Train Loss: 0.0374, Val Loss: 0.2616, Val Accuracy: 0.4813\n","Epoch [1472/10000], Train Loss: 0.0336, Val Loss: 0.2617, Val Accuracy: 0.4813\n","Epoch [1482/10000], Train Loss: 0.0398, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [1492/10000], Train Loss: 0.0343, Val Loss: 0.2616, Val Accuracy: 0.4813\n","Epoch [1502/10000], Train Loss: 0.0384, Val Loss: 0.2621, Val Accuracy: 0.4813\n","Epoch [1512/10000], Train Loss: 0.0340, Val Loss: 0.2596, Val Accuracy: 0.4813\n","Epoch [1522/10000], Train Loss: 0.0400, Val Loss: 0.2678, Val Accuracy: 0.4813\n","Epoch [1532/10000], Train Loss: 0.0334, Val Loss: 0.2602, Val Accuracy: 0.4813\n","Epoch [1542/10000], Train Loss: 0.0373, Val Loss: 0.2623, Val Accuracy: 0.4813\n","Epoch [1552/10000], Train Loss: 0.0371, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [1562/10000], Train Loss: 0.0357, Val Loss: 0.2657, Val Accuracy: 0.4813\n","Epoch [1572/10000], Train Loss: 0.0403, Val Loss: 0.2706, Val Accuracy: 0.4813\n","Epoch [1582/10000], Train Loss: 0.0360, Val Loss: 0.2621, Val Accuracy: 0.4813\n","Epoch [1592/10000], Train Loss: 0.0397, Val Loss: 0.2637, Val Accuracy: 0.4813\n","Epoch [1602/10000], Train Loss: 0.0357, Val Loss: 0.2645, Val Accuracy: 0.4813\n","Epoch [1612/10000], Train Loss: 0.0379, Val Loss: 0.2680, Val Accuracy: 0.4813\n","Epoch [1622/10000], Train Loss: 0.0333, Val Loss: 0.2608, Val Accuracy: 0.4813\n","Epoch [1632/10000], Train Loss: 0.0388, Val Loss: 0.2637, Val Accuracy: 0.4813\n","Epoch [1642/10000], Train Loss: 0.0337, Val Loss: 0.2602, Val Accuracy: 0.4813\n","Epoch [1652/10000], Train Loss: 0.0346, Val Loss: 0.2604, Val Accuracy: 0.4813\n","Epoch [1662/10000], Train Loss: 0.0357, Val Loss: 0.2625, Val Accuracy: 0.4813\n","Epoch [1672/10000], Train Loss: 0.0352, Val Loss: 0.2638, Val Accuracy: 0.4813\n","Epoch [1682/10000], Train Loss: 0.0361, Val Loss: 0.2648, Val Accuracy: 0.4813\n","Epoch [1692/10000], Train Loss: 0.0345, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [1702/10000], Train Loss: 0.0412, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [1712/10000], Train Loss: 0.0334, Val Loss: 0.2599, Val Accuracy: 0.4813\n","Epoch [1722/10000], Train Loss: 0.0351, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [1732/10000], Train Loss: 0.0367, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [1742/10000], Train Loss: 0.0334, Val Loss: 0.2611, Val Accuracy: 0.4813\n","Epoch [1752/10000], Train Loss: 0.0364, Val Loss: 0.2635, Val Accuracy: 0.4813\n","Epoch [1762/10000], Train Loss: 0.0422, Val Loss: 0.2664, Val Accuracy: 0.4813\n","Epoch [1772/10000], Train Loss: 0.0408, Val Loss: 0.2699, Val Accuracy: 0.4813\n","Epoch [1782/10000], Train Loss: 0.0340, Val Loss: 0.2610, Val Accuracy: 0.4813\n","Epoch [1792/10000], Train Loss: 0.0350, Val Loss: 0.2610, Val Accuracy: 0.4813\n","Epoch [1802/10000], Train Loss: 0.0333, Val Loss: 0.2598, Val Accuracy: 0.4813\n","Epoch [1812/10000], Train Loss: 0.0374, Val Loss: 0.2668, Val Accuracy: 0.4813\n","Epoch [1822/10000], Train Loss: 0.0351, Val Loss: 0.2601, Val Accuracy: 0.4813\n","Epoch [1832/10000], Train Loss: 0.0357, Val Loss: 0.2606, Val Accuracy: 0.4813\n","Epoch [1842/10000], Train Loss: 0.0340, Val Loss: 0.2586, Val Accuracy: 0.4813\n","Epoch [1852/10000], Train Loss: 0.0332, Val Loss: 0.2604, Val Accuracy: 0.4813\n","Epoch [1862/10000], Train Loss: 0.0345, Val Loss: 0.2631, Val Accuracy: 0.4813\n","Epoch [1872/10000], Train Loss: 0.0385, Val Loss: 0.2673, Val Accuracy: 0.4813\n","Epoch [1882/10000], Train Loss: 0.0334, Val Loss: 0.2622, Val Accuracy: 0.4813\n","Epoch [1892/10000], Train Loss: 0.0344, Val Loss: 0.2625, Val Accuracy: 0.4813\n","Epoch [1902/10000], Train Loss: 0.0496, Val Loss: 0.2706, Val Accuracy: 0.4813\n","Epoch [1912/10000], Train Loss: 0.0426, Val Loss: 0.2715, Val Accuracy: 0.4813\n","Epoch [1922/10000], Train Loss: 0.0371, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [1932/10000], Train Loss: 0.0337, Val Loss: 0.2621, Val Accuracy: 0.4813\n","Epoch [1942/10000], Train Loss: 0.0350, Val Loss: 0.2622, Val Accuracy: 0.4813\n","Epoch [1952/10000], Train Loss: 0.0333, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [1962/10000], Train Loss: 0.0419, Val Loss: 0.2734, Val Accuracy: 0.4813\n","Epoch [1972/10000], Train Loss: 0.0347, Val Loss: 0.2614, Val Accuracy: 0.4813\n","Epoch [1982/10000], Train Loss: 0.0337, Val Loss: 0.2617, Val Accuracy: 0.4813\n","Epoch [1992/10000], Train Loss: 0.0330, Val Loss: 0.2599, Val Accuracy: 0.4813\n","Epoch [2002/10000], Train Loss: 0.0345, Val Loss: 0.2599, Val Accuracy: 0.4813\n","Epoch [2012/10000], Train Loss: 0.0329, Val Loss: 0.2615, Val Accuracy: 0.4813\n","Epoch [2022/10000], Train Loss: 0.0331, Val Loss: 0.2624, Val Accuracy: 0.4813\n","Epoch [2032/10000], Train Loss: 0.0601, Val Loss: 0.2813, Val Accuracy: 0.4813\n","Epoch [2042/10000], Train Loss: 0.0487, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [2052/10000], Train Loss: 0.0338, Val Loss: 0.2589, Val Accuracy: 0.4813\n","Epoch [2062/10000], Train Loss: 0.0353, Val Loss: 0.2611, Val Accuracy: 0.4813\n","Epoch [2072/10000], Train Loss: 0.0338, Val Loss: 0.2608, Val Accuracy: 0.4813\n","Epoch [2082/10000], Train Loss: 0.0329, Val Loss: 0.2605, Val Accuracy: 0.4813\n","Epoch [2092/10000], Train Loss: 0.0339, Val Loss: 0.2620, Val Accuracy: 0.4813\n","Epoch [2102/10000], Train Loss: 0.0387, Val Loss: 0.2640, Val Accuracy: 0.4813\n","Epoch [2112/10000], Train Loss: 0.0340, Val Loss: 0.2606, Val Accuracy: 0.4813\n","Epoch [2122/10000], Train Loss: 0.0394, Val Loss: 0.2694, Val Accuracy: 0.4813\n","Epoch [2132/10000], Train Loss: 0.0331, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [2142/10000], Train Loss: 0.0405, Val Loss: 0.2663, Val Accuracy: 0.4813\n","Epoch [2152/10000], Train Loss: 0.0334, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [2162/10000], Train Loss: 0.0377, Val Loss: 0.2679, Val Accuracy: 0.4813\n","Epoch [2172/10000], Train Loss: 0.0368, Val Loss: 0.2655, Val Accuracy: 0.4813\n","Epoch [2182/10000], Train Loss: 0.0354, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [2192/10000], Train Loss: 0.0416, Val Loss: 0.2655, Val Accuracy: 0.4813\n","Epoch [2202/10000], Train Loss: 0.0351, Val Loss: 0.2636, Val Accuracy: 0.4813\n","Epoch [2212/10000], Train Loss: 0.0385, Val Loss: 0.2665, Val Accuracy: 0.4813\n","Epoch [2222/10000], Train Loss: 0.0359, Val Loss: 0.2620, Val Accuracy: 0.4813\n","Epoch [2232/10000], Train Loss: 0.0367, Val Loss: 0.2615, Val Accuracy: 0.4813\n","Epoch [2242/10000], Train Loss: 0.0348, Val Loss: 0.2660, Val Accuracy: 0.4813\n","Epoch [2252/10000], Train Loss: 0.0391, Val Loss: 0.2696, Val Accuracy: 0.4813\n","Epoch [2262/10000], Train Loss: 0.0336, Val Loss: 0.2612, Val Accuracy: 0.4813\n","Epoch [2272/10000], Train Loss: 0.0403, Val Loss: 0.2664, Val Accuracy: 0.4813\n","Epoch [2282/10000], Train Loss: 0.0348, Val Loss: 0.2663, Val Accuracy: 0.4813\n","Epoch [2292/10000], Train Loss: 0.0370, Val Loss: 0.2683, Val Accuracy: 0.4813\n","Epoch [2302/10000], Train Loss: 0.0332, Val Loss: 0.2624, Val Accuracy: 0.4813\n","Epoch [2312/10000], Train Loss: 0.0409, Val Loss: 0.2674, Val Accuracy: 0.4813\n","Epoch [2322/10000], Train Loss: 0.0335, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [2332/10000], Train Loss: 0.0351, Val Loss: 0.2655, Val Accuracy: 0.4813\n","Epoch [2342/10000], Train Loss: 0.0361, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [2352/10000], Train Loss: 0.0334, Val Loss: 0.2611, Val Accuracy: 0.4813\n","Epoch [2362/10000], Train Loss: 0.0329, Val Loss: 0.2636, Val Accuracy: 0.4813\n","Epoch [2372/10000], Train Loss: 0.0451, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [2382/10000], Train Loss: 0.0436, Val Loss: 0.2747, Val Accuracy: 0.4813\n","Epoch [2392/10000], Train Loss: 0.0430, Val Loss: 0.2646, Val Accuracy: 0.4813\n","Epoch [2402/10000], Train Loss: 0.0348, Val Loss: 0.2605, Val Accuracy: 0.4813\n","Epoch [2412/10000], Train Loss: 0.0343, Val Loss: 0.2642, Val Accuracy: 0.4813\n","Epoch [2422/10000], Train Loss: 0.0340, Val Loss: 0.2611, Val Accuracy: 0.4813\n","Epoch [2432/10000], Train Loss: 0.0332, Val Loss: 0.2622, Val Accuracy: 0.4813\n","Epoch [2442/10000], Train Loss: 0.0346, Val Loss: 0.2644, Val Accuracy: 0.4813\n","Epoch [2452/10000], Train Loss: 0.0341, Val Loss: 0.2661, Val Accuracy: 0.4813\n","Epoch [2462/10000], Train Loss: 0.0369, Val Loss: 0.2680, Val Accuracy: 0.4813\n","Epoch [2472/10000], Train Loss: 0.0369, Val Loss: 0.2681, Val Accuracy: 0.4813\n","Epoch [2482/10000], Train Loss: 0.0352, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [2492/10000], Train Loss: 0.0379, Val Loss: 0.2654, Val Accuracy: 0.4813\n","Epoch [2502/10000], Train Loss: 0.0345, Val Loss: 0.2661, Val Accuracy: 0.4813\n","Epoch [2512/10000], Train Loss: 0.0395, Val Loss: 0.2702, Val Accuracy: 0.4813\n","Epoch [2522/10000], Train Loss: 0.0357, Val Loss: 0.2646, Val Accuracy: 0.4813\n","Epoch [2532/10000], Train Loss: 0.0351, Val Loss: 0.2622, Val Accuracy: 0.4813\n","Epoch [2542/10000], Train Loss: 0.0353, Val Loss: 0.2680, Val Accuracy: 0.4813\n","Epoch [2552/10000], Train Loss: 0.0379, Val Loss: 0.2701, Val Accuracy: 0.4813\n","Epoch [2562/10000], Train Loss: 0.0328, Val Loss: 0.2618, Val Accuracy: 0.4813\n","Epoch [2572/10000], Train Loss: 0.0346, Val Loss: 0.2642, Val Accuracy: 0.4813\n","Epoch [2582/10000], Train Loss: 0.0431, Val Loss: 0.2651, Val Accuracy: 0.4813\n","Epoch [2592/10000], Train Loss: 0.0341, Val Loss: 0.2613, Val Accuracy: 0.4813\n","Epoch [2602/10000], Train Loss: 0.0350, Val Loss: 0.2637, Val Accuracy: 0.4813\n","Epoch [2612/10000], Train Loss: 0.0342, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [2622/10000], Train Loss: 0.0327, Val Loss: 0.2613, Val Accuracy: 0.4813\n","Epoch [2632/10000], Train Loss: 0.0347, Val Loss: 0.2669, Val Accuracy: 0.4813\n","Epoch [2642/10000], Train Loss: 0.0365, Val Loss: 0.2686, Val Accuracy: 0.4813\n","Epoch [2652/10000], Train Loss: 0.0336, Val Loss: 0.2630, Val Accuracy: 0.4813\n","Epoch [2662/10000], Train Loss: 0.0328, Val Loss: 0.2639, Val Accuracy: 0.4813\n","Epoch [2672/10000], Train Loss: 0.0572, Val Loss: 0.2812, Val Accuracy: 0.4813\n","Epoch [2682/10000], Train Loss: 0.0473, Val Loss: 0.2775, Val Accuracy: 0.4813\n","Epoch [2692/10000], Train Loss: 0.0357, Val Loss: 0.2594, Val Accuracy: 0.4813\n","Epoch [2702/10000], Train Loss: 0.0340, Val Loss: 0.2619, Val Accuracy: 0.4813\n","Epoch [2712/10000], Train Loss: 0.0332, Val Loss: 0.2605, Val Accuracy: 0.4813\n","Epoch [2722/10000], Train Loss: 0.0331, Val Loss: 0.2631, Val Accuracy: 0.4813\n","Epoch [2732/10000], Train Loss: 0.0334, Val Loss: 0.2653, Val Accuracy: 0.4813\n","Epoch [2742/10000], Train Loss: 0.0440, Val Loss: 0.2775, Val Accuracy: 0.4813\n","Epoch [2752/10000], Train Loss: 0.0354, Val Loss: 0.2645, Val Accuracy: 0.4813\n","Epoch [2762/10000], Train Loss: 0.0338, Val Loss: 0.2634, Val Accuracy: 0.4813\n","Epoch [2772/10000], Train Loss: 0.0335, Val Loss: 0.2645, Val Accuracy: 0.4813\n","Epoch [2782/10000], Train Loss: 0.0464, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [2792/10000], Train Loss: 0.0382, Val Loss: 0.2650, Val Accuracy: 0.4813\n","Epoch [2802/10000], Train Loss: 0.0343, Val Loss: 0.2628, Val Accuracy: 0.4813\n","Epoch [2812/10000], Train Loss: 0.0357, Val Loss: 0.2654, Val Accuracy: 0.4813\n","Epoch [2822/10000], Train Loss: 0.0341, Val Loss: 0.2637, Val Accuracy: 0.4813\n","Epoch [2832/10000], Train Loss: 0.0392, Val Loss: 0.2668, Val Accuracy: 0.4813\n","Epoch [2842/10000], Train Loss: 0.0334, Val Loss: 0.2660, Val Accuracy: 0.4813\n","Epoch [2852/10000], Train Loss: 0.0403, Val Loss: 0.2739, Val Accuracy: 0.4813\n","Epoch [2862/10000], Train Loss: 0.0338, Val Loss: 0.2656, Val Accuracy: 0.4813\n","Epoch [2872/10000], Train Loss: 0.0413, Val Loss: 0.2683, Val Accuracy: 0.4813\n","Epoch [2882/10000], Train Loss: 0.0344, Val Loss: 0.2665, Val Accuracy: 0.4813\n","Epoch [2892/10000], Train Loss: 0.0374, Val Loss: 0.2708, Val Accuracy: 0.4813\n","Epoch [2902/10000], Train Loss: 0.0331, Val Loss: 0.2655, Val Accuracy: 0.4813\n","Epoch [2912/10000], Train Loss: 0.0329, Val Loss: 0.2637, Val Accuracy: 0.4813\n","Epoch [2922/10000], Train Loss: 0.0467, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [2932/10000], Train Loss: 0.0464, Val Loss: 0.2800, Val Accuracy: 0.4813\n","Epoch [2942/10000], Train Loss: 0.0366, Val Loss: 0.2610, Val Accuracy: 0.4813\n","Epoch [2952/10000], Train Loss: 0.0337, Val Loss: 0.2620, Val Accuracy: 0.4813\n","Epoch [2962/10000], Train Loss: 0.0337, Val Loss: 0.2643, Val Accuracy: 0.4813\n","Epoch [2972/10000], Train Loss: 0.0327, Val Loss: 0.2623, Val Accuracy: 0.4813\n","Epoch [2982/10000], Train Loss: 0.0335, Val Loss: 0.2640, Val Accuracy: 0.4813\n","Epoch [2992/10000], Train Loss: 0.0379, Val Loss: 0.2666, Val Accuracy: 0.4813\n","Epoch [3002/10000], Train Loss: 0.0327, Val Loss: 0.2626, Val Accuracy: 0.4813\n","Epoch [3012/10000], Train Loss: 0.0362, Val Loss: 0.2682, Val Accuracy: 0.4813\n","Epoch [3022/10000], Train Loss: 0.0374, Val Loss: 0.2700, Val Accuracy: 0.4813\n","Epoch [3032/10000], Train Loss: 0.0363, Val Loss: 0.2661, Val Accuracy: 0.4813\n","Epoch [3042/10000], Train Loss: 0.0347, Val Loss: 0.2645, Val Accuracy: 0.4813\n","Epoch [3052/10000], Train Loss: 0.0345, Val Loss: 0.2687, Val Accuracy: 0.4813\n","Epoch [3062/10000], Train Loss: 0.0404, Val Loss: 0.2749, Val Accuracy: 0.4813\n","Epoch [3072/10000], Train Loss: 0.0341, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [3082/10000], Train Loss: 0.0376, Val Loss: 0.2665, Val Accuracy: 0.4813\n","Epoch [3092/10000], Train Loss: 0.0326, Val Loss: 0.2644, Val Accuracy: 0.4813\n","Epoch [3102/10000], Train Loss: 0.0413, Val Loss: 0.2751, Val Accuracy: 0.4813\n","Epoch [3112/10000], Train Loss: 0.0328, Val Loss: 0.2628, Val Accuracy: 0.4813\n","Epoch [3122/10000], Train Loss: 0.0353, Val Loss: 0.2647, Val Accuracy: 0.4813\n","Epoch [3132/10000], Train Loss: 0.0339, Val Loss: 0.2662, Val Accuracy: 0.4813\n","Epoch [3142/10000], Train Loss: 0.0400, Val Loss: 0.2739, Val Accuracy: 0.4813\n","Epoch [3152/10000], Train Loss: 0.0330, Val Loss: 0.2635, Val Accuracy: 0.4813\n","Epoch [3162/10000], Train Loss: 0.0386, Val Loss: 0.2671, Val Accuracy: 0.4813\n","Epoch [3172/10000], Train Loss: 0.0340, Val Loss: 0.2633, Val Accuracy: 0.4813\n","Epoch [3182/10000], Train Loss: 0.0387, Val Loss: 0.2720, Val Accuracy: 0.4813\n","Epoch [3192/10000], Train Loss: 0.0340, Val Loss: 0.2641, Val Accuracy: 0.4813\n","Epoch [3202/10000], Train Loss: 0.0374, Val Loss: 0.2667, Val Accuracy: 0.4813\n","Epoch [3212/10000], Train Loss: 0.0331, Val Loss: 0.2667, Val Accuracy: 0.4813\n","Epoch [3222/10000], Train Loss: 0.0415, Val Loss: 0.2765, Val Accuracy: 0.4813\n","Epoch [3232/10000], Train Loss: 0.0338, Val Loss: 0.2640, Val Accuracy: 0.4813\n","Epoch [3242/10000], Train Loss: 0.0361, Val Loss: 0.2651, Val Accuracy: 0.4813\n","Epoch [3252/10000], Train Loss: 0.0334, Val Loss: 0.2642, Val Accuracy: 0.4813\n","Epoch [3262/10000], Train Loss: 0.0330, Val Loss: 0.2658, Val Accuracy: 0.4813\n","Epoch [3272/10000], Train Loss: 0.0611, Val Loss: 0.2832, Val Accuracy: 0.4813\n","Epoch [3282/10000], Train Loss: 0.0398, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [3292/10000], Train Loss: 0.0330, Val Loss: 0.2631, Val Accuracy: 0.4813\n","Epoch [3302/10000], Train Loss: 0.0326, Val Loss: 0.2633, Val Accuracy: 0.4813\n","Epoch [3312/10000], Train Loss: 0.0332, Val Loss: 0.2634, Val Accuracy: 0.4813\n","Epoch [3322/10000], Train Loss: 0.0327, Val Loss: 0.2664, Val Accuracy: 0.4813\n","Epoch [3332/10000], Train Loss: 0.0329, Val Loss: 0.2653, Val Accuracy: 0.4813\n","Epoch [3342/10000], Train Loss: 0.0566, Val Loss: 0.2911, Val Accuracy: 0.4813\n","Epoch [3352/10000], Train Loss: 0.0461, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [3362/10000], Train Loss: 0.0343, Val Loss: 0.2671, Val Accuracy: 0.4813\n","Epoch [3372/10000], Train Loss: 0.0327, Val Loss: 0.2641, Val Accuracy: 0.4813\n","Epoch [3382/10000], Train Loss: 0.0324, Val Loss: 0.2653, Val Accuracy: 0.4813\n","Epoch [3392/10000], Train Loss: 0.0340, Val Loss: 0.2656, Val Accuracy: 0.4813\n","Epoch [3402/10000], Train Loss: 0.0324, Val Loss: 0.2649, Val Accuracy: 0.4813\n","Epoch [3412/10000], Train Loss: 0.0355, Val Loss: 0.2710, Val Accuracy: 0.4813\n","Epoch [3422/10000], Train Loss: 0.0359, Val Loss: 0.2697, Val Accuracy: 0.4813\n","Epoch [3432/10000], Train Loss: 0.0370, Val Loss: 0.2662, Val Accuracy: 0.4813\n","Epoch [3442/10000], Train Loss: 0.0357, Val Loss: 0.2689, Val Accuracy: 0.4813\n","Epoch [3452/10000], Train Loss: 0.0328, Val Loss: 0.2657, Val Accuracy: 0.4813\n","Epoch [3462/10000], Train Loss: 0.0400, Val Loss: 0.2702, Val Accuracy: 0.4813\n","Epoch [3472/10000], Train Loss: 0.0326, Val Loss: 0.2668, Val Accuracy: 0.4813\n","Epoch [3482/10000], Train Loss: 0.0387, Val Loss: 0.2723, Val Accuracy: 0.4813\n","Epoch [3492/10000], Train Loss: 0.0330, Val Loss: 0.2652, Val Accuracy: 0.4813\n","Epoch [3502/10000], Train Loss: 0.0344, Val Loss: 0.2671, Val Accuracy: 0.4813\n","Epoch [3512/10000], Train Loss: 0.0463, Val Loss: 0.2739, Val Accuracy: 0.4813\n","Epoch [3522/10000], Train Loss: 0.0402, Val Loss: 0.2763, Val Accuracy: 0.4813\n","Epoch [3532/10000], Train Loss: 0.0359, Val Loss: 0.2671, Val Accuracy: 0.4813\n","Epoch [3542/10000], Train Loss: 0.0349, Val Loss: 0.2692, Val Accuracy: 0.4813\n","Epoch [3552/10000], Train Loss: 0.0324, Val Loss: 0.2664, Val Accuracy: 0.4813\n","Epoch [3562/10000], Train Loss: 0.0345, Val Loss: 0.2653, Val Accuracy: 0.4813\n","Epoch [3572/10000], Train Loss: 0.0411, Val Loss: 0.2691, Val Accuracy: 0.4813\n","Epoch [3582/10000], Train Loss: 0.0382, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [3592/10000], Train Loss: 0.0350, Val Loss: 0.2657, Val Accuracy: 0.4813\n","Epoch [3602/10000], Train Loss: 0.0329, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [3612/10000], Train Loss: 0.0348, Val Loss: 0.2718, Val Accuracy: 0.4813\n","Epoch [3622/10000], Train Loss: 0.0395, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [3632/10000], Train Loss: 0.0364, Val Loss: 0.2685, Val Accuracy: 0.4813\n","Epoch [3642/10000], Train Loss: 0.0327, Val Loss: 0.2657, Val Accuracy: 0.4813\n","Epoch [3652/10000], Train Loss: 0.0328, Val Loss: 0.2664, Val Accuracy: 0.4813\n","Epoch [3662/10000], Train Loss: 0.0502, Val Loss: 0.2883, Val Accuracy: 0.4813\n","Epoch [3672/10000], Train Loss: 0.0444, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [3682/10000], Train Loss: 0.0377, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [3692/10000], Train Loss: 0.0354, Val Loss: 0.2649, Val Accuracy: 0.4813\n","Epoch [3702/10000], Train Loss: 0.0345, Val Loss: 0.2706, Val Accuracy: 0.4813\n","Epoch [3712/10000], Train Loss: 0.0327, Val Loss: 0.2686, Val Accuracy: 0.4813\n","Epoch [3722/10000], Train Loss: 0.0321, Val Loss: 0.2666, Val Accuracy: 0.4813\n","Epoch [3732/10000], Train Loss: 0.0459, Val Loss: 0.2755, Val Accuracy: 0.4813\n","Epoch [3742/10000], Train Loss: 0.0370, Val Loss: 0.2735, Val Accuracy: 0.4813\n","Epoch [3752/10000], Train Loss: 0.0365, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [3762/10000], Train Loss: 0.0336, Val Loss: 0.2662, Val Accuracy: 0.4813\n","Epoch [3772/10000], Train Loss: 0.0328, Val Loss: 0.2669, Val Accuracy: 0.4813\n","Epoch [3782/10000], Train Loss: 0.0324, Val Loss: 0.2681, Val Accuracy: 0.4813\n","Epoch [3792/10000], Train Loss: 0.0332, Val Loss: 0.2670, Val Accuracy: 0.4813\n","Epoch [3802/10000], Train Loss: 0.0475, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [3812/10000], Train Loss: 0.0335, Val Loss: 0.2668, Val Accuracy: 0.4813\n","Epoch [3822/10000], Train Loss: 0.0356, Val Loss: 0.2702, Val Accuracy: 0.4813\n","Epoch [3832/10000], Train Loss: 0.0353, Val Loss: 0.2676, Val Accuracy: 0.4813\n","Epoch [3842/10000], Train Loss: 0.0339, Val Loss: 0.2690, Val Accuracy: 0.4813\n","Epoch [3852/10000], Train Loss: 0.0330, Val Loss: 0.2682, Val Accuracy: 0.4813\n","Epoch [3862/10000], Train Loss: 0.0329, Val Loss: 0.2680, Val Accuracy: 0.4813\n","Epoch [3872/10000], Train Loss: 0.0444, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [3882/10000], Train Loss: 0.0353, Val Loss: 0.2721, Val Accuracy: 0.4813\n","Epoch [3892/10000], Train Loss: 0.0324, Val Loss: 0.2669, Val Accuracy: 0.4813\n","Epoch [3902/10000], Train Loss: 0.0372, Val Loss: 0.2696, Val Accuracy: 0.4813\n","Epoch [3912/10000], Train Loss: 0.0329, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [3922/10000], Train Loss: 0.0395, Val Loss: 0.2785, Val Accuracy: 0.4813\n","Epoch [3932/10000], Train Loss: 0.0326, Val Loss: 0.2669, Val Accuracy: 0.4813\n","Epoch [3942/10000], Train Loss: 0.0372, Val Loss: 0.2686, Val Accuracy: 0.4813\n","Epoch [3952/10000], Train Loss: 0.0349, Val Loss: 0.2665, Val Accuracy: 0.4813\n","Epoch [3962/10000], Train Loss: 0.0369, Val Loss: 0.2727, Val Accuracy: 0.4813\n","Epoch [3972/10000], Train Loss: 0.0328, Val Loss: 0.2674, Val Accuracy: 0.4813\n","Epoch [3982/10000], Train Loss: 0.0326, Val Loss: 0.2695, Val Accuracy: 0.4813\n","Epoch [3992/10000], Train Loss: 0.0597, Val Loss: 0.2858, Val Accuracy: 0.4813\n","Epoch [4002/10000], Train Loss: 0.0450, Val Loss: 0.2836, Val Accuracy: 0.4813\n","Epoch [4012/10000], Train Loss: 0.0337, Val Loss: 0.2633, Val Accuracy: 0.4813\n","Epoch [4022/10000], Train Loss: 0.0328, Val Loss: 0.2666, Val Accuracy: 0.4813\n","Epoch [4032/10000], Train Loss: 0.0323, Val Loss: 0.2666, Val Accuracy: 0.4813\n","Epoch [4042/10000], Train Loss: 0.0337, Val Loss: 0.2684, Val Accuracy: 0.4813\n","Epoch [4052/10000], Train Loss: 0.0321, Val Loss: 0.2690, Val Accuracy: 0.4813\n","Epoch [4062/10000], Train Loss: 0.0338, Val Loss: 0.2697, Val Accuracy: 0.4813\n","Epoch [4072/10000], Train Loss: 0.0501, Val Loss: 0.2869, Val Accuracy: 0.4813\n","Epoch [4082/10000], Train Loss: 0.0444, Val Loss: 0.2708, Val Accuracy: 0.4813\n","Epoch [4092/10000], Train Loss: 0.0378, Val Loss: 0.2688, Val Accuracy: 0.4813\n","Epoch [4102/10000], Train Loss: 0.0359, Val Loss: 0.2671, Val Accuracy: 0.4813\n","Epoch [4112/10000], Train Loss: 0.0333, Val Loss: 0.2694, Val Accuracy: 0.4813\n","Epoch [4122/10000], Train Loss: 0.0342, Val Loss: 0.2698, Val Accuracy: 0.4813\n","Epoch [4132/10000], Train Loss: 0.0354, Val Loss: 0.2734, Val Accuracy: 0.4813\n","Epoch [4142/10000], Train Loss: 0.0370, Val Loss: 0.2743, Val Accuracy: 0.4813\n","Epoch [4152/10000], Train Loss: 0.0331, Val Loss: 0.2688, Val Accuracy: 0.4813\n","Epoch [4162/10000], Train Loss: 0.0409, Val Loss: 0.2736, Val Accuracy: 0.4813\n","Epoch [4172/10000], Train Loss: 0.0327, Val Loss: 0.2679, Val Accuracy: 0.4813\n","Epoch [4182/10000], Train Loss: 0.0373, Val Loss: 0.2762, Val Accuracy: 0.4813\n","Epoch [4192/10000], Train Loss: 0.0351, Val Loss: 0.2699, Val Accuracy: 0.4813\n","Epoch [4202/10000], Train Loss: 0.0335, Val Loss: 0.2682, Val Accuracy: 0.4813\n","Epoch [4212/10000], Train Loss: 0.0423, Val Loss: 0.2726, Val Accuracy: 0.4813\n","Epoch [4222/10000], Train Loss: 0.0367, Val Loss: 0.2754, Val Accuracy: 0.4813\n","Epoch [4232/10000], Train Loss: 0.0329, Val Loss: 0.2697, Val Accuracy: 0.4813\n","Epoch [4242/10000], Train Loss: 0.0379, Val Loss: 0.2699, Val Accuracy: 0.4813\n","Epoch [4252/10000], Train Loss: 0.0327, Val Loss: 0.2659, Val Accuracy: 0.4813\n","Epoch [4262/10000], Train Loss: 0.0364, Val Loss: 0.2762, Val Accuracy: 0.4813\n","Epoch [4272/10000], Train Loss: 0.0347, Val Loss: 0.2729, Val Accuracy: 0.4813\n","Epoch [4282/10000], Train Loss: 0.0340, Val Loss: 0.2707, Val Accuracy: 0.4813\n","Epoch [4292/10000], Train Loss: 0.0425, Val Loss: 0.2733, Val Accuracy: 0.4813\n","Epoch [4302/10000], Train Loss: 0.0348, Val Loss: 0.2733, Val Accuracy: 0.4813\n","Epoch [4312/10000], Train Loss: 0.0332, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [4322/10000], Train Loss: 0.0362, Val Loss: 0.2697, Val Accuracy: 0.4813\n","Epoch [4332/10000], Train Loss: 0.0339, Val Loss: 0.2686, Val Accuracy: 0.4813\n","Epoch [4342/10000], Train Loss: 0.0349, Val Loss: 0.2745, Val Accuracy: 0.4813\n","Epoch [4352/10000], Train Loss: 0.0373, Val Loss: 0.2766, Val Accuracy: 0.4813\n","Epoch [4362/10000], Train Loss: 0.0337, Val Loss: 0.2701, Val Accuracy: 0.4813\n","Epoch [4372/10000], Train Loss: 0.0360, Val Loss: 0.2691, Val Accuracy: 0.4813\n","Epoch [4382/10000], Train Loss: 0.0346, Val Loss: 0.2685, Val Accuracy: 0.4813\n","Epoch [4392/10000], Train Loss: 0.0331, Val Loss: 0.2699, Val Accuracy: 0.4813\n","Epoch [4402/10000], Train Loss: 0.0529, Val Loss: 0.2831, Val Accuracy: 0.4813\n","Epoch [4412/10000], Train Loss: 0.0425, Val Loss: 0.2825, Val Accuracy: 0.4813\n","Epoch [4422/10000], Train Loss: 0.0367, Val Loss: 0.2694, Val Accuracy: 0.4813\n","Epoch [4432/10000], Train Loss: 0.0342, Val Loss: 0.2728, Val Accuracy: 0.4813\n","Epoch [4442/10000], Train Loss: 0.0328, Val Loss: 0.2714, Val Accuracy: 0.4813\n","Epoch [4452/10000], Train Loss: 0.0346, Val Loss: 0.2692, Val Accuracy: 0.4813\n","Epoch [4462/10000], Train Loss: 0.0387, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [4472/10000], Train Loss: 0.0353, Val Loss: 0.2730, Val Accuracy: 0.4813\n","Epoch [4482/10000], Train Loss: 0.0335, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [4492/10000], Train Loss: 0.0328, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [4502/10000], Train Loss: 0.0374, Val Loss: 0.2774, Val Accuracy: 0.4813\n","Epoch [4512/10000], Train Loss: 0.0345, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [4522/10000], Train Loss: 0.0333, Val Loss: 0.2704, Val Accuracy: 0.4813\n","Epoch [4532/10000], Train Loss: 0.0331, Val Loss: 0.2661, Val Accuracy: 0.4813\n","Epoch [4542/10000], Train Loss: 0.0322, Val Loss: 0.2678, Val Accuracy: 0.4813\n","Epoch [4552/10000], Train Loss: 0.0342, Val Loss: 0.2729, Val Accuracy: 0.4813\n","Epoch [4562/10000], Train Loss: 0.0338, Val Loss: 0.2728, Val Accuracy: 0.4813\n","Epoch [4572/10000], Train Loss: 0.0338, Val Loss: 0.2736, Val Accuracy: 0.4813\n","Epoch [4582/10000], Train Loss: 0.0409, Val Loss: 0.2828, Val Accuracy: 0.4813\n","Epoch [4592/10000], Train Loss: 0.0348, Val Loss: 0.2693, Val Accuracy: 0.4813\n","Epoch [4602/10000], Train Loss: 0.0352, Val Loss: 0.2685, Val Accuracy: 0.4813\n","Epoch [4612/10000], Train Loss: 0.0319, Val Loss: 0.2704, Val Accuracy: 0.4813\n","Epoch [4622/10000], Train Loss: 0.0346, Val Loss: 0.2772, Val Accuracy: 0.4813\n","Epoch [4632/10000], Train Loss: 0.0408, Val Loss: 0.2797, Val Accuracy: 0.4813\n","Epoch [4642/10000], Train Loss: 0.0329, Val Loss: 0.2694, Val Accuracy: 0.4813\n","Epoch [4652/10000], Train Loss: 0.0345, Val Loss: 0.2673, Val Accuracy: 0.4813\n","Epoch [4662/10000], Train Loss: 0.0329, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [4672/10000], Train Loss: 0.0322, Val Loss: 0.2695, Val Accuracy: 0.4813\n","Epoch [4682/10000], Train Loss: 0.0329, Val Loss: 0.2702, Val Accuracy: 0.4813\n","Epoch [4692/10000], Train Loss: 0.0390, Val Loss: 0.2706, Val Accuracy: 0.4813\n","Epoch [4702/10000], Train Loss: 0.0323, Val Loss: 0.2681, Val Accuracy: 0.4813\n","Epoch [4712/10000], Train Loss: 0.0384, Val Loss: 0.2790, Val Accuracy: 0.4813\n","Epoch [4722/10000], Train Loss: 0.0324, Val Loss: 0.2709, Val Accuracy: 0.4813\n","Epoch [4732/10000], Train Loss: 0.0384, Val Loss: 0.2724, Val Accuracy: 0.4813\n","Epoch [4742/10000], Train Loss: 0.0324, Val Loss: 0.2691, Val Accuracy: 0.4813\n","Epoch [4752/10000], Train Loss: 0.0396, Val Loss: 0.2801, Val Accuracy: 0.4813\n","Epoch [4762/10000], Train Loss: 0.0319, Val Loss: 0.2696, Val Accuracy: 0.4813\n","Epoch [4772/10000], Train Loss: 0.0381, Val Loss: 0.2724, Val Accuracy: 0.4813\n","Epoch [4782/10000], Train Loss: 0.0343, Val Loss: 0.2748, Val Accuracy: 0.4813\n","Epoch [4792/10000], Train Loss: 0.0341, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [4802/10000], Train Loss: 0.0335, Val Loss: 0.2708, Val Accuracy: 0.4813\n","Epoch [4812/10000], Train Loss: 0.0420, Val Loss: 0.2742, Val Accuracy: 0.4813\n","Epoch [4822/10000], Train Loss: 0.0351, Val Loss: 0.2765, Val Accuracy: 0.4813\n","Epoch [4832/10000], Train Loss: 0.0328, Val Loss: 0.2723, Val Accuracy: 0.4813\n","Epoch [4842/10000], Train Loss: 0.0345, Val Loss: 0.2719, Val Accuracy: 0.4813\n","Epoch [4852/10000], Train Loss: 0.0399, Val Loss: 0.2742, Val Accuracy: 0.4813\n","Epoch [4862/10000], Train Loss: 0.0342, Val Loss: 0.2746, Val Accuracy: 0.4813\n","Epoch [4872/10000], Train Loss: 0.0355, Val Loss: 0.2741, Val Accuracy: 0.4813\n","Epoch [4882/10000], Train Loss: 0.0352, Val Loss: 0.2706, Val Accuracy: 0.4813\n","Epoch [4892/10000], Train Loss: 0.0341, Val Loss: 0.2714, Val Accuracy: 0.4813\n","Epoch [4902/10000], Train Loss: 0.0326, Val Loss: 0.2733, Val Accuracy: 0.4813\n","Epoch [4912/10000], Train Loss: 0.0478, Val Loss: 0.2907, Val Accuracy: 0.4813\n","Epoch [4922/10000], Train Loss: 0.0393, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [4932/10000], Train Loss: 0.0352, Val Loss: 0.2765, Val Accuracy: 0.4813\n","Epoch [4942/10000], Train Loss: 0.0318, Val Loss: 0.2718, Val Accuracy: 0.4813\n","Epoch [4952/10000], Train Loss: 0.0342, Val Loss: 0.2722, Val Accuracy: 0.4813\n","Epoch [4962/10000], Train Loss: 0.0384, Val Loss: 0.2720, Val Accuracy: 0.4813\n","Epoch [4972/10000], Train Loss: 0.0369, Val Loss: 0.2800, Val Accuracy: 0.4813\n","Epoch [4982/10000], Train Loss: 0.0326, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [4992/10000], Train Loss: 0.0383, Val Loss: 0.2746, Val Accuracy: 0.4813\n","Epoch [5002/10000], Train Loss: 0.0319, Val Loss: 0.2721, Val Accuracy: 0.4813\n","Epoch [5012/10000], Train Loss: 0.0366, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [5022/10000], Train Loss: 0.0364, Val Loss: 0.2770, Val Accuracy: 0.4813\n","Epoch [5032/10000], Train Loss: 0.0320, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [5042/10000], Train Loss: 0.0429, Val Loss: 0.2784, Val Accuracy: 0.4813\n","Epoch [5052/10000], Train Loss: 0.0340, Val Loss: 0.2764, Val Accuracy: 0.4813\n","Epoch [5062/10000], Train Loss: 0.0325, Val Loss: 0.2717, Val Accuracy: 0.4813\n","Epoch [5072/10000], Train Loss: 0.0339, Val Loss: 0.2723, Val Accuracy: 0.4813\n","Epoch [5082/10000], Train Loss: 0.0320, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [5092/10000], Train Loss: 0.0368, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [5102/10000], Train Loss: 0.0330, Val Loss: 0.2745, Val Accuracy: 0.4813\n","Epoch [5112/10000], Train Loss: 0.0363, Val Loss: 0.2723, Val Accuracy: 0.4813\n","Epoch [5122/10000], Train Loss: 0.0345, Val Loss: 0.2737, Val Accuracy: 0.4813\n","Epoch [5132/10000], Train Loss: 0.0341, Val Loss: 0.2754, Val Accuracy: 0.4813\n","Epoch [5142/10000], Train Loss: 0.0320, Val Loss: 0.2745, Val Accuracy: 0.4813\n","Epoch [5152/10000], Train Loss: 0.0386, Val Loss: 0.2757, Val Accuracy: 0.4813\n","Epoch [5162/10000], Train Loss: 0.0327, Val Loss: 0.2727, Val Accuracy: 0.4813\n","Epoch [5172/10000], Train Loss: 0.0343, Val Loss: 0.2714, Val Accuracy: 0.4813\n","Epoch [5182/10000], Train Loss: 0.0337, Val Loss: 0.2742, Val Accuracy: 0.4813\n","Epoch [5192/10000], Train Loss: 0.0320, Val Loss: 0.2720, Val Accuracy: 0.4813\n","Epoch [5202/10000], Train Loss: 0.0332, Val Loss: 0.2720, Val Accuracy: 0.4813\n","Epoch [5212/10000], Train Loss: 0.0319, Val Loss: 0.2730, Val Accuracy: 0.4813\n","Epoch [5222/10000], Train Loss: 0.0403, Val Loss: 0.2828, Val Accuracy: 0.4813\n","Epoch [5232/10000], Train Loss: 0.0327, Val Loss: 0.2710, Val Accuracy: 0.4813\n","Epoch [5242/10000], Train Loss: 0.0333, Val Loss: 0.2705, Val Accuracy: 0.4813\n","Epoch [5252/10000], Train Loss: 0.0359, Val Loss: 0.2798, Val Accuracy: 0.4813\n","Epoch [5262/10000], Train Loss: 0.0326, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [5272/10000], Train Loss: 0.0331, Val Loss: 0.2716, Val Accuracy: 0.4813\n","Epoch [5282/10000], Train Loss: 0.0429, Val Loss: 0.2764, Val Accuracy: 0.4813\n","Epoch [5292/10000], Train Loss: 0.0349, Val Loss: 0.2777, Val Accuracy: 0.4813\n","Epoch [5302/10000], Train Loss: 0.0319, Val Loss: 0.2734, Val Accuracy: 0.4813\n","Epoch [5312/10000], Train Loss: 0.0348, Val Loss: 0.2747, Val Accuracy: 0.4813\n","Epoch [5322/10000], Train Loss: 0.0396, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [5332/10000], Train Loss: 0.0330, Val Loss: 0.2756, Val Accuracy: 0.4813\n","Epoch [5342/10000], Train Loss: 0.0369, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [5352/10000], Train Loss: 0.0335, Val Loss: 0.2722, Val Accuracy: 0.4813\n","Epoch [5362/10000], Train Loss: 0.0392, Val Loss: 0.2740, Val Accuracy: 0.4813\n","Epoch [5372/10000], Train Loss: 0.0335, Val Loss: 0.2707, Val Accuracy: 0.4813\n","Epoch [5382/10000], Train Loss: 0.0379, Val Loss: 0.2783, Val Accuracy: 0.4813\n","Epoch [5392/10000], Train Loss: 0.0342, Val Loss: 0.2738, Val Accuracy: 0.4813\n","Epoch [5402/10000], Train Loss: 0.0340, Val Loss: 0.2726, Val Accuracy: 0.4813\n","Epoch [5412/10000], Train Loss: 0.0394, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [5422/10000], Train Loss: 0.0380, Val Loss: 0.2813, Val Accuracy: 0.4813\n","Epoch [5432/10000], Train Loss: 0.0328, Val Loss: 0.2725, Val Accuracy: 0.4813\n","Epoch [5442/10000], Train Loss: 0.0350, Val Loss: 0.2728, Val Accuracy: 0.4813\n","Epoch [5452/10000], Train Loss: 0.0335, Val Loss: 0.2775, Val Accuracy: 0.4813\n","Epoch [5462/10000], Train Loss: 0.0386, Val Loss: 0.2837, Val Accuracy: 0.4813\n","Epoch [5472/10000], Train Loss: 0.0322, Val Loss: 0.2743, Val Accuracy: 0.4813\n","Epoch [5482/10000], Train Loss: 0.0394, Val Loss: 0.2760, Val Accuracy: 0.4813\n","Epoch [5492/10000], Train Loss: 0.0321, Val Loss: 0.2721, Val Accuracy: 0.4813\n","Epoch [5502/10000], Train Loss: 0.0373, Val Loss: 0.2776, Val Accuracy: 0.4813\n","Epoch [5512/10000], Train Loss: 0.0325, Val Loss: 0.2717, Val Accuracy: 0.4813\n","Epoch [5522/10000], Train Loss: 0.0385, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [5532/10000], Train Loss: 0.0323, Val Loss: 0.2750, Val Accuracy: 0.4813\n","Epoch [5542/10000], Train Loss: 0.0349, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [5552/10000], Train Loss: 0.0377, Val Loss: 0.2821, Val Accuracy: 0.4813\n","Epoch [5562/10000], Train Loss: 0.0378, Val Loss: 0.2763, Val Accuracy: 0.4813\n","Epoch [5572/10000], Train Loss: 0.0346, Val Loss: 0.2777, Val Accuracy: 0.4813\n","Epoch [5582/10000], Train Loss: 0.0322, Val Loss: 0.2744, Val Accuracy: 0.4813\n","Epoch [5592/10000], Train Loss: 0.0332, Val Loss: 0.2729, Val Accuracy: 0.4813\n","Epoch [5602/10000], Train Loss: 0.0427, Val Loss: 0.2789, Val Accuracy: 0.4813\n","Epoch [5612/10000], Train Loss: 0.0375, Val Loss: 0.2816, Val Accuracy: 0.4813\n","Epoch [5622/10000], Train Loss: 0.0323, Val Loss: 0.2732, Val Accuracy: 0.4813\n","Epoch [5632/10000], Train Loss: 0.0348, Val Loss: 0.2745, Val Accuracy: 0.4813\n","Epoch [5642/10000], Train Loss: 0.0321, Val Loss: 0.2747, Val Accuracy: 0.4813\n","Epoch [5652/10000], Train Loss: 0.0337, Val Loss: 0.2731, Val Accuracy: 0.4813\n","Epoch [5662/10000], Train Loss: 0.0497, Val Loss: 0.2802, Val Accuracy: 0.4813\n","Epoch [5672/10000], Train Loss: 0.0377, Val Loss: 0.2778, Val Accuracy: 0.4813\n","Epoch [5682/10000], Train Loss: 0.0352, Val Loss: 0.2739, Val Accuracy: 0.4813\n","Epoch [5692/10000], Train Loss: 0.0338, Val Loss: 0.2757, Val Accuracy: 0.4813\n","Epoch [5702/10000], Train Loss: 0.0320, Val Loss: 0.2713, Val Accuracy: 0.4813\n","Epoch [5712/10000], Train Loss: 0.0347, Val Loss: 0.2750, Val Accuracy: 0.4813\n","Epoch [5722/10000], Train Loss: 0.0372, Val Loss: 0.2762, Val Accuracy: 0.4813\n","Epoch [5732/10000], Train Loss: 0.0368, Val Loss: 0.2822, Val Accuracy: 0.4813\n","Epoch [5742/10000], Train Loss: 0.0318, Val Loss: 0.2752, Val Accuracy: 0.4813\n","Epoch [5752/10000], Train Loss: 0.0358, Val Loss: 0.2744, Val Accuracy: 0.4813\n","Epoch [5762/10000], Train Loss: 0.0343, Val Loss: 0.2729, Val Accuracy: 0.4813\n","Epoch [5772/10000], Train Loss: 0.0315, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [5782/10000], Train Loss: 0.0345, Val Loss: 0.2806, Val Accuracy: 0.4813\n","Epoch [5792/10000], Train Loss: 0.0347, Val Loss: 0.2785, Val Accuracy: 0.4813\n","Epoch [5802/10000], Train Loss: 0.0355, Val Loss: 0.2799, Val Accuracy: 0.4813\n","Epoch [5812/10000], Train Loss: 0.0348, Val Loss: 0.2744, Val Accuracy: 0.4813\n","Epoch [5822/10000], Train Loss: 0.0334, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [5832/10000], Train Loss: 0.0322, Val Loss: 0.2731, Val Accuracy: 0.4813\n","Epoch [5842/10000], Train Loss: 0.0330, Val Loss: 0.2742, Val Accuracy: 0.4813\n","Epoch [5852/10000], Train Loss: 0.0360, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [5862/10000], Train Loss: 0.0333, Val Loss: 0.2741, Val Accuracy: 0.4813\n","Epoch [5872/10000], Train Loss: 0.0330, Val Loss: 0.2738, Val Accuracy: 0.4813\n","Epoch [5882/10000], Train Loss: 0.0421, Val Loss: 0.2807, Val Accuracy: 0.4813\n","Epoch [5892/10000], Train Loss: 0.0326, Val Loss: 0.2765, Val Accuracy: 0.4813\n","Epoch [5902/10000], Train Loss: 0.0353, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [5912/10000], Train Loss: 0.0325, Val Loss: 0.2774, Val Accuracy: 0.4813\n","Epoch [5922/10000], Train Loss: 0.0317, Val Loss: 0.2747, Val Accuracy: 0.4813\n","Epoch [5932/10000], Train Loss: 0.0379, Val Loss: 0.2837, Val Accuracy: 0.4813\n","Epoch [5942/10000], Train Loss: 0.0390, Val Loss: 0.2738, Val Accuracy: 0.4813\n","Epoch [5952/10000], Train Loss: 0.0407, Val Loss: 0.2813, Val Accuracy: 0.4813\n","Epoch [5962/10000], Train Loss: 0.0328, Val Loss: 0.2726, Val Accuracy: 0.4813\n","Epoch [5972/10000], Train Loss: 0.0320, Val Loss: 0.2737, Val Accuracy: 0.4813\n","Epoch [5982/10000], Train Loss: 0.0317, Val Loss: 0.2727, Val Accuracy: 0.4813\n","Epoch [5992/10000], Train Loss: 0.0316, Val Loss: 0.2748, Val Accuracy: 0.4813\n","Epoch [6002/10000], Train Loss: 0.0336, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [6012/10000], Train Loss: 0.0393, Val Loss: 0.2773, Val Accuracy: 0.4813\n","Epoch [6022/10000], Train Loss: 0.0367, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [6032/10000], Train Loss: 0.0322, Val Loss: 0.2720, Val Accuracy: 0.4813\n","Epoch [6042/10000], Train Loss: 0.0347, Val Loss: 0.2760, Val Accuracy: 0.4813\n","Epoch [6052/10000], Train Loss: 0.0360, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [6062/10000], Train Loss: 0.0315, Val Loss: 0.2738, Val Accuracy: 0.4813\n","Epoch [6072/10000], Train Loss: 0.0423, Val Loss: 0.2883, Val Accuracy: 0.4813\n","Epoch [6082/10000], Train Loss: 0.0347, Val Loss: 0.2733, Val Accuracy: 0.4813\n","Epoch [6092/10000], Train Loss: 0.0322, Val Loss: 0.2756, Val Accuracy: 0.4813\n","Epoch [6102/10000], Train Loss: 0.0320, Val Loss: 0.2763, Val Accuracy: 0.4813\n","Epoch [6112/10000], Train Loss: 0.0347, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [6122/10000], Train Loss: 0.0323, Val Loss: 0.2740, Val Accuracy: 0.4813\n","Epoch [6132/10000], Train Loss: 0.0317, Val Loss: 0.2744, Val Accuracy: 0.4813\n","Epoch [6142/10000], Train Loss: 0.0446, Val Loss: 0.2918, Val Accuracy: 0.4813\n","Epoch [6152/10000], Train Loss: 0.0404, Val Loss: 0.2779, Val Accuracy: 0.4813\n","Epoch [6162/10000], Train Loss: 0.0387, Val Loss: 0.2833, Val Accuracy: 0.4813\n","Epoch [6172/10000], Train Loss: 0.0354, Val Loss: 0.2718, Val Accuracy: 0.4813\n","Epoch [6182/10000], Train Loss: 0.0323, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [6192/10000], Train Loss: 0.0317, Val Loss: 0.2763, Val Accuracy: 0.4813\n","Epoch [6202/10000], Train Loss: 0.0343, Val Loss: 0.2746, Val Accuracy: 0.4813\n","Epoch [6212/10000], Train Loss: 0.0365, Val Loss: 0.2750, Val Accuracy: 0.4813\n","Epoch [6222/10000], Train Loss: 0.0363, Val Loss: 0.2820, Val Accuracy: 0.4813\n","Epoch [6232/10000], Train Loss: 0.0321, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [6242/10000], Train Loss: 0.0377, Val Loss: 0.2789, Val Accuracy: 0.4813\n","Epoch [6252/10000], Train Loss: 0.0315, Val Loss: 0.2753, Val Accuracy: 0.4813\n","Epoch [6262/10000], Train Loss: 0.0338, Val Loss: 0.2767, Val Accuracy: 0.4813\n","Epoch [6272/10000], Train Loss: 0.0437, Val Loss: 0.2883, Val Accuracy: 0.4813\n","Epoch [6282/10000], Train Loss: 0.0363, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [6292/10000], Train Loss: 0.0317, Val Loss: 0.2759, Val Accuracy: 0.4813\n","Epoch [6302/10000], Train Loss: 0.0349, Val Loss: 0.2830, Val Accuracy: 0.4813\n","Epoch [6312/10000], Train Loss: 0.0339, Val Loss: 0.2791, Val Accuracy: 0.4813\n","Epoch [6322/10000], Train Loss: 0.0334, Val Loss: 0.2773, Val Accuracy: 0.4813\n","Epoch [6332/10000], Train Loss: 0.0394, Val Loss: 0.2874, Val Accuracy: 0.4813\n","Epoch [6342/10000], Train Loss: 0.0325, Val Loss: 0.2775, Val Accuracy: 0.4813\n","Epoch [6352/10000], Train Loss: 0.0365, Val Loss: 0.2782, Val Accuracy: 0.4813\n","Epoch [6362/10000], Train Loss: 0.0320, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [6372/10000], Train Loss: 0.0404, Val Loss: 0.2858, Val Accuracy: 0.4813\n","Epoch [6382/10000], Train Loss: 0.0322, Val Loss: 0.2736, Val Accuracy: 0.4813\n","Epoch [6392/10000], Train Loss: 0.0353, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [6402/10000], Train Loss: 0.0334, Val Loss: 0.2764, Val Accuracy: 0.4813\n","Epoch [6412/10000], Train Loss: 0.0325, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [6422/10000], Train Loss: 0.0475, Val Loss: 0.2842, Val Accuracy: 0.4813\n","Epoch [6432/10000], Train Loss: 0.0400, Val Loss: 0.2867, Val Accuracy: 0.4813\n","Epoch [6442/10000], Train Loss: 0.0355, Val Loss: 0.2743, Val Accuracy: 0.4813\n","Epoch [6452/10000], Train Loss: 0.0325, Val Loss: 0.2764, Val Accuracy: 0.4813\n","Epoch [6462/10000], Train Loss: 0.0321, Val Loss: 0.2751, Val Accuracy: 0.4813\n","Epoch [6472/10000], Train Loss: 0.0315, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [6482/10000], Train Loss: 0.0337, Val Loss: 0.2757, Val Accuracy: 0.4813\n","Epoch [6492/10000], Train Loss: 0.0375, Val Loss: 0.2726, Val Accuracy: 0.4813\n","Epoch [6502/10000], Train Loss: 0.0337, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [6512/10000], Train Loss: 0.0354, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [6522/10000], Train Loss: 0.0329, Val Loss: 0.2748, Val Accuracy: 0.4813\n","Epoch [6532/10000], Train Loss: 0.0317, Val Loss: 0.2746, Val Accuracy: 0.4813\n","Epoch [6542/10000], Train Loss: 0.0329, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [6552/10000], Train Loss: 0.0331, Val Loss: 0.2797, Val Accuracy: 0.4813\n","Epoch [6562/10000], Train Loss: 0.0347, Val Loss: 0.2822, Val Accuracy: 0.4813\n","Epoch [6572/10000], Train Loss: 0.0374, Val Loss: 0.2820, Val Accuracy: 0.4813\n","Epoch [6582/10000], Train Loss: 0.0341, Val Loss: 0.2764, Val Accuracy: 0.4813\n","Epoch [6592/10000], Train Loss: 0.0352, Val Loss: 0.2740, Val Accuracy: 0.4813\n","Epoch [6602/10000], Train Loss: 0.0334, Val Loss: 0.2795, Val Accuracy: 0.4813\n","Epoch [6612/10000], Train Loss: 0.0378, Val Loss: 0.2851, Val Accuracy: 0.4813\n","Epoch [6622/10000], Train Loss: 0.0319, Val Loss: 0.2794, Val Accuracy: 0.4813\n","Epoch [6632/10000], Train Loss: 0.0323, Val Loss: 0.2743, Val Accuracy: 0.4813\n","Epoch [6642/10000], Train Loss: 0.0510, Val Loss: 0.2876, Val Accuracy: 0.4813\n","Epoch [6652/10000], Train Loss: 0.0394, Val Loss: 0.2867, Val Accuracy: 0.4813\n","Epoch [6662/10000], Train Loss: 0.0354, Val Loss: 0.2758, Val Accuracy: 0.4813\n","Epoch [6672/10000], Train Loss: 0.0326, Val Loss: 0.2798, Val Accuracy: 0.4813\n","Epoch [6682/10000], Train Loss: 0.0327, Val Loss: 0.2799, Val Accuracy: 0.4813\n","Epoch [6692/10000], Train Loss: 0.0316, Val Loss: 0.2746, Val Accuracy: 0.4813\n","Epoch [6702/10000], Train Loss: 0.0341, Val Loss: 0.2785, Val Accuracy: 0.4813\n","Epoch [6712/10000], Train Loss: 0.0397, Val Loss: 0.2793, Val Accuracy: 0.4813\n","Epoch [6722/10000], Train Loss: 0.0328, Val Loss: 0.2758, Val Accuracy: 0.4813\n","Epoch [6732/10000], Train Loss: 0.0330, Val Loss: 0.2778, Val Accuracy: 0.4813\n","Epoch [6742/10000], Train Loss: 0.0322, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [6752/10000], Train Loss: 0.0312, Val Loss: 0.2760, Val Accuracy: 0.4813\n","Epoch [6762/10000], Train Loss: 0.0319, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [6772/10000], Train Loss: 0.0352, Val Loss: 0.2839, Val Accuracy: 0.4813\n","Epoch [6782/10000], Train Loss: 0.0348, Val Loss: 0.2819, Val Accuracy: 0.4813\n","Epoch [6792/10000], Train Loss: 0.0336, Val Loss: 0.2734, Val Accuracy: 0.4813\n","Epoch [6802/10000], Train Loss: 0.0365, Val Loss: 0.2774, Val Accuracy: 0.4813\n","Epoch [6812/10000], Train Loss: 0.0313, Val Loss: 0.2770, Val Accuracy: 0.4813\n","Epoch [6822/10000], Train Loss: 0.0313, Val Loss: 0.2780, Val Accuracy: 0.4813\n","Epoch [6832/10000], Train Loss: 0.0569, Val Loss: 0.3067, Val Accuracy: 0.4813\n","Epoch [6842/10000], Train Loss: 0.0466, Val Loss: 0.2830, Val Accuracy: 0.4813\n","Epoch [6852/10000], Train Loss: 0.0338, Val Loss: 0.2777, Val Accuracy: 0.4813\n","Epoch [6862/10000], Train Loss: 0.0322, Val Loss: 0.2780, Val Accuracy: 0.4813\n","Epoch [6872/10000], Train Loss: 0.0312, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [6882/10000], Train Loss: 0.0317, Val Loss: 0.2750, Val Accuracy: 0.4813\n","Epoch [6892/10000], Train Loss: 0.0323, Val Loss: 0.2776, Val Accuracy: 0.4813\n","Epoch [6902/10000], Train Loss: 0.0374, Val Loss: 0.2784, Val Accuracy: 0.4813\n","Epoch [6912/10000], Train Loss: 0.0325, Val Loss: 0.2754, Val Accuracy: 0.4813\n","Epoch [6922/10000], Train Loss: 0.0358, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [6932/10000], Train Loss: 0.0319, Val Loss: 0.2781, Val Accuracy: 0.4813\n","Epoch [6942/10000], Train Loss: 0.0315, Val Loss: 0.2789, Val Accuracy: 0.4813\n","Epoch [6952/10000], Train Loss: 0.0329, Val Loss: 0.2766, Val Accuracy: 0.4813\n","Epoch [6962/10000], Train Loss: 0.0460, Val Loss: 0.2799, Val Accuracy: 0.4813\n","Epoch [6972/10000], Train Loss: 0.0329, Val Loss: 0.2734, Val Accuracy: 0.4813\n","Epoch [6982/10000], Train Loss: 0.0348, Val Loss: 0.2789, Val Accuracy: 0.4813\n","Epoch [6992/10000], Train Loss: 0.0329, Val Loss: 0.2773, Val Accuracy: 0.4813\n","Epoch [7002/10000], Train Loss: 0.0312, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [7012/10000], Train Loss: 0.0323, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [7022/10000], Train Loss: 0.0370, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [7032/10000], Train Loss: 0.0314, Val Loss: 0.2756, Val Accuracy: 0.4813\n","Epoch [7042/10000], Train Loss: 0.0323, Val Loss: 0.2785, Val Accuracy: 0.4813\n","Epoch [7052/10000], Train Loss: 0.0486, Val Loss: 0.2858, Val Accuracy: 0.4813\n","Epoch [7062/10000], Train Loss: 0.0402, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [7072/10000], Train Loss: 0.0353, Val Loss: 0.2742, Val Accuracy: 0.4813\n","Epoch [7082/10000], Train Loss: 0.0312, Val Loss: 0.2778, Val Accuracy: 0.4813\n","Epoch [7092/10000], Train Loss: 0.0335, Val Loss: 0.2818, Val Accuracy: 0.4813\n","Epoch [7102/10000], Train Loss: 0.0340, Val Loss: 0.2816, Val Accuracy: 0.4813\n","Epoch [7112/10000], Train Loss: 0.0326, Val Loss: 0.2797, Val Accuracy: 0.4813\n","Epoch [7122/10000], Train Loss: 0.0393, Val Loss: 0.2887, Val Accuracy: 0.4813\n","Epoch [7132/10000], Train Loss: 0.0318, Val Loss: 0.2795, Val Accuracy: 0.4813\n","Epoch [7142/10000], Train Loss: 0.0353, Val Loss: 0.2777, Val Accuracy: 0.4813\n","Epoch [7152/10000], Train Loss: 0.0326, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [7162/10000], Train Loss: 0.0379, Val Loss: 0.2892, Val Accuracy: 0.4813\n","Epoch [7172/10000], Train Loss: 0.0320, Val Loss: 0.2798, Val Accuracy: 0.4813\n","Epoch [7182/10000], Train Loss: 0.0362, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [7192/10000], Train Loss: 0.0337, Val Loss: 0.2761, Val Accuracy: 0.4813\n","Epoch [7202/10000], Train Loss: 0.0337, Val Loss: 0.2812, Val Accuracy: 0.4813\n","Epoch [7212/10000], Train Loss: 0.0335, Val Loss: 0.2836, Val Accuracy: 0.4813\n","Epoch [7222/10000], Train Loss: 0.0341, Val Loss: 0.2840, Val Accuracy: 0.4813\n","Epoch [7232/10000], Train Loss: 0.0397, Val Loss: 0.2852, Val Accuracy: 0.4813\n","Epoch [7242/10000], Train Loss: 0.0338, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [7252/10000], Train Loss: 0.0347, Val Loss: 0.2787, Val Accuracy: 0.4813\n","Epoch [7262/10000], Train Loss: 0.0318, Val Loss: 0.2800, Val Accuracy: 0.4813\n","Epoch [7272/10000], Train Loss: 0.0406, Val Loss: 0.2923, Val Accuracy: 0.4813\n","Epoch [7282/10000], Train Loss: 0.0321, Val Loss: 0.2786, Val Accuracy: 0.4813\n","Epoch [7292/10000], Train Loss: 0.0324, Val Loss: 0.2769, Val Accuracy: 0.4813\n","Epoch [7302/10000], Train Loss: 0.0342, Val Loss: 0.2810, Val Accuracy: 0.4813\n","Epoch [7312/10000], Train Loss: 0.0331, Val Loss: 0.2808, Val Accuracy: 0.4813\n","Epoch [7322/10000], Train Loss: 0.0333, Val Loss: 0.2831, Val Accuracy: 0.4813\n","Epoch [7332/10000], Train Loss: 0.0413, Val Loss: 0.2924, Val Accuracy: 0.4813\n","Epoch [7342/10000], Train Loss: 0.0348, Val Loss: 0.2800, Val Accuracy: 0.4813\n","Epoch [7352/10000], Train Loss: 0.0319, Val Loss: 0.2767, Val Accuracy: 0.4813\n","Epoch [7362/10000], Train Loss: 0.0321, Val Loss: 0.2788, Val Accuracy: 0.4813\n","Epoch [7372/10000], Train Loss: 0.0432, Val Loss: 0.2924, Val Accuracy: 0.4813\n","Epoch [7382/10000], Train Loss: 0.0332, Val Loss: 0.2771, Val Accuracy: 0.4813\n","Epoch [7392/10000], Train Loss: 0.0329, Val Loss: 0.2793, Val Accuracy: 0.4813\n","Epoch [7402/10000], Train Loss: 0.0321, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [7412/10000], Train Loss: 0.0420, Val Loss: 0.2920, Val Accuracy: 0.4813\n","Epoch [7422/10000], Train Loss: 0.0323, Val Loss: 0.2786, Val Accuracy: 0.4813\n","Epoch [7432/10000], Train Loss: 0.0341, Val Loss: 0.2788, Val Accuracy: 0.4813\n","Epoch [7442/10000], Train Loss: 0.0325, Val Loss: 0.2815, Val Accuracy: 0.4813\n","Epoch [7452/10000], Train Loss: 0.0366, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [7462/10000], Train Loss: 0.0324, Val Loss: 0.2791, Val Accuracy: 0.4813\n","Epoch [7472/10000], Train Loss: 0.0319, Val Loss: 0.2819, Val Accuracy: 0.4813\n","Epoch [7482/10000], Train Loss: 0.0483, Val Loss: 0.3007, Val Accuracy: 0.4813\n","Epoch [7492/10000], Train Loss: 0.0369, Val Loss: 0.2797, Val Accuracy: 0.4813\n","Epoch [7502/10000], Train Loss: 0.0329, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [7512/10000], Train Loss: 0.0320, Val Loss: 0.2796, Val Accuracy: 0.4813\n","Epoch [7522/10000], Train Loss: 0.0317, Val Loss: 0.2803, Val Accuracy: 0.4813\n","Epoch [7532/10000], Train Loss: 0.0366, Val Loss: 0.2807, Val Accuracy: 0.4813\n","Epoch [7542/10000], Train Loss: 0.0332, Val Loss: 0.2777, Val Accuracy: 0.4813\n","Epoch [7552/10000], Train Loss: 0.0342, Val Loss: 0.2843, Val Accuracy: 0.4813\n","Epoch [7562/10000], Train Loss: 0.0336, Val Loss: 0.2768, Val Accuracy: 0.4813\n","Epoch [7572/10000], Train Loss: 0.0332, Val Loss: 0.2794, Val Accuracy: 0.4813\n","Epoch [7582/10000], Train Loss: 0.0338, Val Loss: 0.2810, Val Accuracy: 0.4813\n","Epoch [7592/10000], Train Loss: 0.0374, Val Loss: 0.2826, Val Accuracy: 0.4813\n","Epoch [7602/10000], Train Loss: 0.0313, Val Loss: 0.2775, Val Accuracy: 0.4813\n","Epoch [7612/10000], Train Loss: 0.0357, Val Loss: 0.2857, Val Accuracy: 0.4813\n","Epoch [7622/10000], Train Loss: 0.0333, Val Loss: 0.2840, Val Accuracy: 0.4813\n","Epoch [7632/10000], Train Loss: 0.0365, Val Loss: 0.2830, Val Accuracy: 0.4813\n","Epoch [7642/10000], Train Loss: 0.0317, Val Loss: 0.2801, Val Accuracy: 0.4813\n","Epoch [7652/10000], Train Loss: 0.0317, Val Loss: 0.2797, Val Accuracy: 0.4813\n","Epoch [7662/10000], Train Loss: 0.0494, Val Loss: 0.2987, Val Accuracy: 0.4813\n","Epoch [7672/10000], Train Loss: 0.0402, Val Loss: 0.2819, Val Accuracy: 0.4813\n","Epoch [7682/10000], Train Loss: 0.0346, Val Loss: 0.2828, Val Accuracy: 0.4813\n","Epoch [7692/10000], Train Loss: 0.0313, Val Loss: 0.2779, Val Accuracy: 0.4813\n","Epoch [7702/10000], Train Loss: 0.0337, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [7712/10000], Train Loss: 0.0335, Val Loss: 0.2816, Val Accuracy: 0.4813\n","Epoch [7722/10000], Train Loss: 0.0341, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [7732/10000], Train Loss: 0.0373, Val Loss: 0.2808, Val Accuracy: 0.4813\n","Epoch [7742/10000], Train Loss: 0.0309, Val Loss: 0.2806, Val Accuracy: 0.4813\n","Epoch [7752/10000], Train Loss: 0.0320, Val Loss: 0.2818, Val Accuracy: 0.4813\n","Epoch [7762/10000], Train Loss: 0.0534, Val Loss: 0.3036, Val Accuracy: 0.4813\n","Epoch [7772/10000], Train Loss: 0.0422, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [7782/10000], Train Loss: 0.0347, Val Loss: 0.2835, Val Accuracy: 0.4813\n","Epoch [7792/10000], Train Loss: 0.0327, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [7802/10000], Train Loss: 0.0323, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [7812/10000], Train Loss: 0.0319, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [7822/10000], Train Loss: 0.0366, Val Loss: 0.2885, Val Accuracy: 0.4813\n","Epoch [7832/10000], Train Loss: 0.0327, Val Loss: 0.2837, Val Accuracy: 0.4813\n","Epoch [7842/10000], Train Loss: 0.0347, Val Loss: 0.2827, Val Accuracy: 0.4813\n","Epoch [7852/10000], Train Loss: 0.0312, Val Loss: 0.2788, Val Accuracy: 0.4813\n","Epoch [7862/10000], Train Loss: 0.0314, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [7872/10000], Train Loss: 0.0483, Val Loss: 0.3003, Val Accuracy: 0.4813\n","Epoch [7882/10000], Train Loss: 0.0427, Val Loss: 0.2856, Val Accuracy: 0.4813\n","Epoch [7892/10000], Train Loss: 0.0373, Val Loss: 0.2858, Val Accuracy: 0.4813\n","Epoch [7902/10000], Train Loss: 0.0341, Val Loss: 0.2786, Val Accuracy: 0.4813\n","Epoch [7912/10000], Train Loss: 0.0327, Val Loss: 0.2839, Val Accuracy: 0.4813\n","Epoch [7922/10000], Train Loss: 0.0311, Val Loss: 0.2784, Val Accuracy: 0.4813\n","Epoch [7932/10000], Train Loss: 0.0322, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [7942/10000], Train Loss: 0.0435, Val Loss: 0.2882, Val Accuracy: 0.4813\n","Epoch [7952/10000], Train Loss: 0.0355, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [7962/10000], Train Loss: 0.0314, Val Loss: 0.2785, Val Accuracy: 0.4813\n","Epoch [7972/10000], Train Loss: 0.0338, Val Loss: 0.2817, Val Accuracy: 0.4813\n","Epoch [7982/10000], Train Loss: 0.0322, Val Loss: 0.2821, Val Accuracy: 0.4813\n","Epoch [7992/10000], Train Loss: 0.0327, Val Loss: 0.2803, Val Accuracy: 0.4813\n","Epoch [8002/10000], Train Loss: 0.0451, Val Loss: 0.2846, Val Accuracy: 0.4813\n","Epoch [8012/10000], Train Loss: 0.0386, Val Loss: 0.2843, Val Accuracy: 0.4813\n","Epoch [8022/10000], Train Loss: 0.0330, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [8032/10000], Train Loss: 0.0318, Val Loss: 0.2795, Val Accuracy: 0.4813\n","Epoch [8042/10000], Train Loss: 0.0310, Val Loss: 0.2824, Val Accuracy: 0.4813\n","Epoch [8052/10000], Train Loss: 0.0334, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [8062/10000], Train Loss: 0.0379, Val Loss: 0.2879, Val Accuracy: 0.4813\n","Epoch [8072/10000], Train Loss: 0.0316, Val Loss: 0.2789, Val Accuracy: 0.4813\n","Epoch [8082/10000], Train Loss: 0.0323, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [8092/10000], Train Loss: 0.0314, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [8102/10000], Train Loss: 0.0310, Val Loss: 0.2799, Val Accuracy: 0.4813\n","Epoch [8112/10000], Train Loss: 0.0316, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [8122/10000], Train Loss: 0.0373, Val Loss: 0.2836, Val Accuracy: 0.4813\n","Epoch [8132/10000], Train Loss: 0.0311, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [8142/10000], Train Loss: 0.0375, Val Loss: 0.2893, Val Accuracy: 0.4813\n","Epoch [8152/10000], Train Loss: 0.0309, Val Loss: 0.2824, Val Accuracy: 0.4813\n","Epoch [8162/10000], Train Loss: 0.0362, Val Loss: 0.2838, Val Accuracy: 0.4813\n","Epoch [8172/10000], Train Loss: 0.0317, Val Loss: 0.2802, Val Accuracy: 0.4813\n","Epoch [8182/10000], Train Loss: 0.0325, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [8192/10000], Train Loss: 0.0408, Val Loss: 0.2952, Val Accuracy: 0.4813\n","Epoch [8202/10000], Train Loss: 0.0326, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [8212/10000], Train Loss: 0.0340, Val Loss: 0.2817, Val Accuracy: 0.4813\n","Epoch [8222/10000], Train Loss: 0.0351, Val Loss: 0.2878, Val Accuracy: 0.4813\n","Epoch [8232/10000], Train Loss: 0.0314, Val Loss: 0.2840, Val Accuracy: 0.4813\n","Epoch [8242/10000], Train Loss: 0.0308, Val Loss: 0.2804, Val Accuracy: 0.4813\n","Epoch [8252/10000], Train Loss: 0.0332, Val Loss: 0.2832, Val Accuracy: 0.4813\n","Epoch [8262/10000], Train Loss: 0.0377, Val Loss: 0.2792, Val Accuracy: 0.4813\n","Epoch [8272/10000], Train Loss: 0.0364, Val Loss: 0.2800, Val Accuracy: 0.4813\n","Epoch [8282/10000], Train Loss: 0.0336, Val Loss: 0.2829, Val Accuracy: 0.4813\n","Epoch [8292/10000], Train Loss: 0.0313, Val Loss: 0.2801, Val Accuracy: 0.4813\n","Epoch [8302/10000], Train Loss: 0.0312, Val Loss: 0.2809, Val Accuracy: 0.4813\n","Epoch [8312/10000], Train Loss: 0.0307, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [8322/10000], Train Loss: 0.0329, Val Loss: 0.2825, Val Accuracy: 0.4813\n","Epoch [8332/10000], Train Loss: 0.0362, Val Loss: 0.2841, Val Accuracy: 0.4813\n","Epoch [8342/10000], Train Loss: 0.0313, Val Loss: 0.2810, Val Accuracy: 0.4813\n","Epoch [8352/10000], Train Loss: 0.0390, Val Loss: 0.2896, Val Accuracy: 0.4813\n","Epoch [8362/10000], Train Loss: 0.0310, Val Loss: 0.2812, Val Accuracy: 0.4813\n","Epoch [8372/10000], Train Loss: 0.0356, Val Loss: 0.2838, Val Accuracy: 0.4813\n","Epoch [8382/10000], Train Loss: 0.0324, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [8392/10000], Train Loss: 0.0315, Val Loss: 0.2816, Val Accuracy: 0.4813\n","Epoch [8402/10000], Train Loss: 0.0498, Val Loss: 0.2914, Val Accuracy: 0.4813\n","Epoch [8412/10000], Train Loss: 0.0392, Val Loss: 0.2879, Val Accuracy: 0.4813\n","Epoch [8422/10000], Train Loss: 0.0339, Val Loss: 0.2813, Val Accuracy: 0.4813\n","Epoch [8432/10000], Train Loss: 0.0311, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [8442/10000], Train Loss: 0.0321, Val Loss: 0.2833, Val Accuracy: 0.4813\n","Epoch [8452/10000], Train Loss: 0.0340, Val Loss: 0.2856, Val Accuracy: 0.4813\n","Epoch [8462/10000], Train Loss: 0.0368, Val Loss: 0.2884, Val Accuracy: 0.4813\n","Epoch [8472/10000], Train Loss: 0.0314, Val Loss: 0.2817, Val Accuracy: 0.4813\n","Epoch [8482/10000], Train Loss: 0.0385, Val Loss: 0.2874, Val Accuracy: 0.4813\n","Epoch [8492/10000], Train Loss: 0.0318, Val Loss: 0.2832, Val Accuracy: 0.4813\n","Epoch [8502/10000], Train Loss: 0.0339, Val Loss: 0.2895, Val Accuracy: 0.4813\n","Epoch [8512/10000], Train Loss: 0.0381, Val Loss: 0.2931, Val Accuracy: 0.4813\n","Epoch [8522/10000], Train Loss: 0.0317, Val Loss: 0.2828, Val Accuracy: 0.4813\n","Epoch [8532/10000], Train Loss: 0.0359, Val Loss: 0.2841, Val Accuracy: 0.4813\n","Epoch [8542/10000], Train Loss: 0.0318, Val Loss: 0.2820, Val Accuracy: 0.4813\n","Epoch [8552/10000], Train Loss: 0.0306, Val Loss: 0.2827, Val Accuracy: 0.4813\n","Epoch [8562/10000], Train Loss: 0.0403, Val Loss: 0.2957, Val Accuracy: 0.4813\n","Epoch [8572/10000], Train Loss: 0.0383, Val Loss: 0.2875, Val Accuracy: 0.4813\n","Epoch [8582/10000], Train Loss: 0.0387, Val Loss: 0.2919, Val Accuracy: 0.4813\n","Epoch [8592/10000], Train Loss: 0.0339, Val Loss: 0.2782, Val Accuracy: 0.4813\n","Epoch [8602/10000], Train Loss: 0.0315, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [8612/10000], Train Loss: 0.0309, Val Loss: 0.2812, Val Accuracy: 0.4813\n","Epoch [8622/10000], Train Loss: 0.0313, Val Loss: 0.2835, Val Accuracy: 0.4813\n","Epoch [8632/10000], Train Loss: 0.0326, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [8642/10000], Train Loss: 0.0368, Val Loss: 0.2908, Val Accuracy: 0.4813\n","Epoch [8652/10000], Train Loss: 0.0307, Val Loss: 0.2821, Val Accuracy: 0.4813\n","Epoch [8662/10000], Train Loss: 0.0369, Val Loss: 0.2863, Val Accuracy: 0.4813\n","Epoch [8672/10000], Train Loss: 0.0324, Val Loss: 0.2841, Val Accuracy: 0.4813\n","Epoch [8682/10000], Train Loss: 0.0337, Val Loss: 0.2868, Val Accuracy: 0.4813\n","Epoch [8692/10000], Train Loss: 0.0365, Val Loss: 0.2891, Val Accuracy: 0.4813\n","Epoch [8702/10000], Train Loss: 0.0318, Val Loss: 0.2816, Val Accuracy: 0.4813\n","Epoch [8712/10000], Train Loss: 0.0352, Val Loss: 0.2861, Val Accuracy: 0.4813\n","Epoch [8722/10000], Train Loss: 0.0358, Val Loss: 0.2864, Val Accuracy: 0.4813\n","Epoch [8732/10000], Train Loss: 0.0347, Val Loss: 0.2889, Val Accuracy: 0.4813\n","Epoch [8742/10000], Train Loss: 0.0316, Val Loss: 0.2837, Val Accuracy: 0.4813\n","Epoch [8752/10000], Train Loss: 0.0322, Val Loss: 0.2839, Val Accuracy: 0.4813\n","Epoch [8762/10000], Train Loss: 0.0412, Val Loss: 0.2890, Val Accuracy: 0.4813\n","Epoch [8772/10000], Train Loss: 0.0343, Val Loss: 0.2864, Val Accuracy: 0.4813\n","Epoch [8782/10000], Train Loss: 0.0309, Val Loss: 0.2835, Val Accuracy: 0.4813\n","Epoch [8792/10000], Train Loss: 0.0327, Val Loss: 0.2845, Val Accuracy: 0.4813\n","Epoch [8802/10000], Train Loss: 0.0363, Val Loss: 0.2867, Val Accuracy: 0.4813\n","Epoch [8812/10000], Train Loss: 0.0321, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [8822/10000], Train Loss: 0.0308, Val Loss: 0.2802, Val Accuracy: 0.4813\n","Epoch [8832/10000], Train Loss: 0.0315, Val Loss: 0.2857, Val Accuracy: 0.4813\n","Epoch [8842/10000], Train Loss: 0.0658, Val Loss: 0.3035, Val Accuracy: 0.4813\n","Epoch [8852/10000], Train Loss: 0.0348, Val Loss: 0.2802, Val Accuracy: 0.4813\n","Epoch [8862/10000], Train Loss: 0.0343, Val Loss: 0.2845, Val Accuracy: 0.4813\n","Epoch [8872/10000], Train Loss: 0.0327, Val Loss: 0.2805, Val Accuracy: 0.4813\n","Epoch [8882/10000], Train Loss: 0.0311, Val Loss: 0.2826, Val Accuracy: 0.4813\n","Epoch [8892/10000], Train Loss: 0.0308, Val Loss: 0.2827, Val Accuracy: 0.4813\n","Epoch [8902/10000], Train Loss: 0.0306, Val Loss: 0.2822, Val Accuracy: 0.4813\n","Epoch [8912/10000], Train Loss: 0.0336, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [8922/10000], Train Loss: 0.0369, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [8932/10000], Train Loss: 0.0354, Val Loss: 0.2863, Val Accuracy: 0.4813\n","Epoch [8942/10000], Train Loss: 0.0313, Val Loss: 0.2814, Val Accuracy: 0.4813\n","Epoch [8952/10000], Train Loss: 0.0337, Val Loss: 0.2858, Val Accuracy: 0.4813\n","Epoch [8962/10000], Train Loss: 0.0372, Val Loss: 0.2884, Val Accuracy: 0.4813\n","Epoch [8972/10000], Train Loss: 0.0309, Val Loss: 0.2851, Val Accuracy: 0.4813\n","Epoch [8982/10000], Train Loss: 0.0361, Val Loss: 0.2885, Val Accuracy: 0.4813\n","Epoch [8992/10000], Train Loss: 0.0332, Val Loss: 0.2847, Val Accuracy: 0.4813\n","Epoch [9002/10000], Train Loss: 0.0340, Val Loss: 0.2844, Val Accuracy: 0.4813\n","Epoch [9012/10000], Train Loss: 0.0337, Val Loss: 0.2838, Val Accuracy: 0.4813\n","Epoch [9022/10000], Train Loss: 0.0322, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [9032/10000], Train Loss: 0.0378, Val Loss: 0.2930, Val Accuracy: 0.4813\n","Epoch [9042/10000], Train Loss: 0.0323, Val Loss: 0.2819, Val Accuracy: 0.4813\n","Epoch [9052/10000], Train Loss: 0.0350, Val Loss: 0.2864, Val Accuracy: 0.4813\n","Epoch [9062/10000], Train Loss: 0.0309, Val Loss: 0.2847, Val Accuracy: 0.4813\n","Epoch [9072/10000], Train Loss: 0.0322, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [9082/10000], Train Loss: 0.0500, Val Loss: 0.3028, Val Accuracy: 0.4813\n","Epoch [9092/10000], Train Loss: 0.0376, Val Loss: 0.2828, Val Accuracy: 0.4813\n","Epoch [9102/10000], Train Loss: 0.0325, Val Loss: 0.2845, Val Accuracy: 0.4813\n","Epoch [9112/10000], Train Loss: 0.0321, Val Loss: 0.2855, Val Accuracy: 0.4813\n","Epoch [9122/10000], Train Loss: 0.0315, Val Loss: 0.2864, Val Accuracy: 0.4813\n","Epoch [9132/10000], Train Loss: 0.0321, Val Loss: 0.2842, Val Accuracy: 0.4813\n","Epoch [9142/10000], Train Loss: 0.0359, Val Loss: 0.2909, Val Accuracy: 0.4813\n","Epoch [9152/10000], Train Loss: 0.0346, Val Loss: 0.2899, Val Accuracy: 0.4813\n","Epoch [9162/10000], Train Loss: 0.0351, Val Loss: 0.2871, Val Accuracy: 0.4813\n","Epoch [9172/10000], Train Loss: 0.0315, Val Loss: 0.2855, Val Accuracy: 0.4813\n","Epoch [9182/10000], Train Loss: 0.0310, Val Loss: 0.2847, Val Accuracy: 0.4813\n","Epoch [9192/10000], Train Loss: 0.0445, Val Loss: 0.2990, Val Accuracy: 0.4813\n","Epoch [9202/10000], Train Loss: 0.0370, Val Loss: 0.2835, Val Accuracy: 0.4813\n","Epoch [9212/10000], Train Loss: 0.0325, Val Loss: 0.2831, Val Accuracy: 0.4813\n","Epoch [9222/10000], Train Loss: 0.0310, Val Loss: 0.2833, Val Accuracy: 0.4813\n","Epoch [9232/10000], Train Loss: 0.0312, Val Loss: 0.2851, Val Accuracy: 0.4813\n","Epoch [9242/10000], Train Loss: 0.0368, Val Loss: 0.2878, Val Accuracy: 0.4813\n","Epoch [9252/10000], Train Loss: 0.0322, Val Loss: 0.2810, Val Accuracy: 0.4813\n","Epoch [9262/10000], Train Loss: 0.0338, Val Loss: 0.2878, Val Accuracy: 0.4813\n","Epoch [9272/10000], Train Loss: 0.0318, Val Loss: 0.2863, Val Accuracy: 0.4813\n","Epoch [9282/10000], Train Loss: 0.0327, Val Loss: 0.2902, Val Accuracy: 0.4813\n","Epoch [9292/10000], Train Loss: 0.0414, Val Loss: 0.2990, Val Accuracy: 0.4813\n","Epoch [9302/10000], Train Loss: 0.0317, Val Loss: 0.2864, Val Accuracy: 0.4813\n","Epoch [9312/10000], Train Loss: 0.0353, Val Loss: 0.2872, Val Accuracy: 0.4813\n","Epoch [9322/10000], Train Loss: 0.0308, Val Loss: 0.2852, Val Accuracy: 0.4813\n","Epoch [9332/10000], Train Loss: 0.0384, Val Loss: 0.2925, Val Accuracy: 0.4813\n","Epoch [9342/10000], Train Loss: 0.0311, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [9352/10000], Train Loss: 0.0340, Val Loss: 0.2850, Val Accuracy: 0.4813\n","Epoch [9362/10000], Train Loss: 0.0356, Val Loss: 0.2877, Val Accuracy: 0.4813\n","Epoch [9372/10000], Train Loss: 0.0317, Val Loss: 0.2868, Val Accuracy: 0.4813\n","Epoch [9382/10000], Train Loss: 0.0370, Val Loss: 0.2939, Val Accuracy: 0.4813\n","Epoch [9392/10000], Train Loss: 0.0310, Val Loss: 0.2850, Val Accuracy: 0.4813\n","Epoch [9402/10000], Train Loss: 0.0308, Val Loss: 0.2846, Val Accuracy: 0.4813\n","Epoch [9412/10000], Train Loss: 0.0489, Val Loss: 0.2983, Val Accuracy: 0.4813\n","Epoch [9422/10000], Train Loss: 0.0383, Val Loss: 0.2965, Val Accuracy: 0.4813\n","Epoch [9432/10000], Train Loss: 0.0340, Val Loss: 0.2854, Val Accuracy: 0.4813\n","Epoch [9442/10000], Train Loss: 0.0310, Val Loss: 0.2869, Val Accuracy: 0.4813\n","Epoch [9452/10000], Train Loss: 0.0312, Val Loss: 0.2850, Val Accuracy: 0.4813\n","Epoch [9462/10000], Train Loss: 0.0311, Val Loss: 0.2870, Val Accuracy: 0.4813\n","Epoch [9472/10000], Train Loss: 0.0429, Val Loss: 0.3014, Val Accuracy: 0.4813\n","Epoch [9482/10000], Train Loss: 0.0327, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [9492/10000], Train Loss: 0.0306, Val Loss: 0.2846, Val Accuracy: 0.4813\n","Epoch [9502/10000], Train Loss: 0.0334, Val Loss: 0.2874, Val Accuracy: 0.4813\n","Epoch [9512/10000], Train Loss: 0.0317, Val Loss: 0.2868, Val Accuracy: 0.4813\n","Epoch [9522/10000], Train Loss: 0.0315, Val Loss: 0.2884, Val Accuracy: 0.4813\n","Epoch [9532/10000], Train Loss: 0.0429, Val Loss: 0.3018, Val Accuracy: 0.4813\n","Epoch [9542/10000], Train Loss: 0.0320, Val Loss: 0.2854, Val Accuracy: 0.4813\n","Epoch [9552/10000], Train Loss: 0.0307, Val Loss: 0.2840, Val Accuracy: 0.4813\n","Epoch [9562/10000], Train Loss: 0.0319, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [9572/10000], Train Loss: 0.0303, Val Loss: 0.2854, Val Accuracy: 0.4813\n","Epoch [9582/10000], Train Loss: 0.0325, Val Loss: 0.2867, Val Accuracy: 0.4813\n","Epoch [9592/10000], Train Loss: 0.0454, Val Loss: 0.2933, Val Accuracy: 0.4813\n","Epoch [9602/10000], Train Loss: 0.0343, Val Loss: 0.2899, Val Accuracy: 0.4813\n","Epoch [9612/10000], Train Loss: 0.0324, Val Loss: 0.2845, Val Accuracy: 0.4813\n","Epoch [9622/10000], Train Loss: 0.0324, Val Loss: 0.2867, Val Accuracy: 0.4813\n","Epoch [9632/10000], Train Loss: 0.0303, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [9642/10000], Train Loss: 0.0331, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [9652/10000], Train Loss: 0.0384, Val Loss: 0.2859, Val Accuracy: 0.4813\n","Epoch [9662/10000], Train Loss: 0.0353, Val Loss: 0.2892, Val Accuracy: 0.4813\n","Epoch [9672/10000], Train Loss: 0.0306, Val Loss: 0.2852, Val Accuracy: 0.4813\n","Epoch [9682/10000], Train Loss: 0.0314, Val Loss: 0.2860, Val Accuracy: 0.4813\n","Epoch [9692/10000], Train Loss: 0.0450, Val Loss: 0.2978, Val Accuracy: 0.4813\n","Epoch [9702/10000], Train Loss: 0.0365, Val Loss: 0.2940, Val Accuracy: 0.4813\n","Epoch [9712/10000], Train Loss: 0.0317, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [9722/10000], Train Loss: 0.0323, Val Loss: 0.2875, Val Accuracy: 0.4813\n","Epoch [9732/10000], Train Loss: 0.0312, Val Loss: 0.2900, Val Accuracy: 0.4813\n","Epoch [9742/10000], Train Loss: 0.0330, Val Loss: 0.2897, Val Accuracy: 0.4813\n","Epoch [9752/10000], Train Loss: 0.0413, Val Loss: 0.2972, Val Accuracy: 0.4813\n","Epoch [9762/10000], Train Loss: 0.0355, Val Loss: 0.2848, Val Accuracy: 0.4813\n","Epoch [9772/10000], Train Loss: 0.0323, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [9782/10000], Train Loss: 0.0319, Val Loss: 0.2876, Val Accuracy: 0.4813\n","Epoch [9792/10000], Train Loss: 0.0310, Val Loss: 0.2861, Val Accuracy: 0.4813\n","Epoch [9802/10000], Train Loss: 0.0312, Val Loss: 0.2859, Val Accuracy: 0.4813\n","Epoch [9812/10000], Train Loss: 0.0453, Val Loss: 0.2963, Val Accuracy: 0.4813\n","Epoch [9822/10000], Train Loss: 0.0348, Val Loss: 0.2950, Val Accuracy: 0.4813\n","Epoch [9832/10000], Train Loss: 0.0309, Val Loss: 0.2863, Val Accuracy: 0.4813\n","Epoch [9842/10000], Train Loss: 0.0309, Val Loss: 0.2855, Val Accuracy: 0.4813\n","Epoch [9852/10000], Train Loss: 0.0306, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [9862/10000], Train Loss: 0.0356, Val Loss: 0.2942, Val Accuracy: 0.4813\n","Epoch [9872/10000], Train Loss: 0.0310, Val Loss: 0.2862, Val Accuracy: 0.4813\n","Epoch [9882/10000], Train Loss: 0.0319, Val Loss: 0.2846, Val Accuracy: 0.4813\n","Epoch [9892/10000], Train Loss: 0.0310, Val Loss: 0.2849, Val Accuracy: 0.4813\n","Epoch [9902/10000], Train Loss: 0.0311, Val Loss: 0.2860, Val Accuracy: 0.4813\n","Epoch [9912/10000], Train Loss: 0.0308, Val Loss: 0.2874, Val Accuracy: 0.4813\n","Epoch [9922/10000], Train Loss: 0.0348, Val Loss: 0.2942, Val Accuracy: 0.4813\n","Epoch [9932/10000], Train Loss: 0.0333, Val Loss: 0.2930, Val Accuracy: 0.4813\n","Epoch [9942/10000], Train Loss: 0.0327, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [9952/10000], Train Loss: 0.0337, Val Loss: 0.2873, Val Accuracy: 0.4813\n","Epoch [9962/10000], Train Loss: 0.0303, Val Loss: 0.2865, Val Accuracy: 0.4813\n","Epoch [9972/10000], Train Loss: 0.0349, Val Loss: 0.2925, Val Accuracy: 0.4813\n","Epoch [9982/10000], Train Loss: 0.0351, Val Loss: 0.2910, Val Accuracy: 0.4813\n","Epoch [9992/10000], Train Loss: 0.0327, Val Loss: 0.2848, Val Accuracy: 0.4813\n","Elapsed time is 23.293772 seconds.\n"]}],"source":["t = pytictoc.TicToc()\n","t.tic()\n","\n","for epoch in range(epochs):\n","    epoch += 1\n","    y_pred = model.forward(x_train).squeeze()\n","    loss = loss_function(y_pred, y_train)\n","    final_losses.append(loss)\n","\n","    val_accuracy, val_loss = evaluate_model(model, x_test, y_test)\n","\n","    val_pred = model.forward(x_test).squeeze()\n","    val_loss = loss_function(val_pred, y_test)\n","\n","    if epoch%10 == 1:\n","        # print('Epoch number: {} and the loss: {}'.format(epoch, loss.item()))\n","        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","t.toc()"]},{"cell_type":"markdown","id":"c732d5a9","metadata":{},"source":["Feature Scaling: Machine learning models often perform better when the input features are on a similar scale. You could consider scaling your input features using techniques such as Min-Max Scaling or Standard Scaling.\n","\n","Feature Selection: Not all features are equally relevant for predicting your target variable. You could use feature selection techniques to identify and select the most informative features. This can also help to reduce overfitting.\n","\n","Feature Engineering: You could create new features that might be informative for your prediction task. For example, you could calculate moving averages, price change percentages, or other technical indicators.\n","\n","Time Series Considerations: Stock prices are a time series, and time series data has some unique characteristics that you might want to consider. For example, you could include lagged features (i.e., the value of the stock price or other features at previous time steps) as input features.\n","\n","Model Selection: Different models have different strengths and weaknesses, and some models might be better suited to your task than others. You could consider trying out different types of models (e.g., linear models, decision tree-based models, neural networks) to see which one performs best.\n","\n","Hyperparameter Tuning: The performance of your model can often be improved by tuning its hyperparameters. For neural networks, important hyperparameters include the learning rate, the number of layers, and the number of units in each layer."]},{"cell_type":"markdown","id":"a33a8e8b-45b2-4a18-b842-693674d3009b","metadata":{"id":"a33a8e8b-45b2-4a18-b842-693674d3009b"},"source":["## Training Setup"]},{"cell_type":"markdown","id":"47b0341d-252c-4782-9705-385ef965fbf3","metadata":{"id":"47b0341d-252c-4782-9705-385ef965fbf3"},"source":["> We attempt to convert the numpy and pandas series we have currently used for our dataset into tensors\n","Pandas dataframes and Numpy Arrays are used before this step for data exploration and manipulation but the deep learning library pytorch performs operations on tensors."]},{"cell_type":"code","execution_count":null,"id":"61e75887","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508","metadata":{"id":"1586c57d-2bec-4a3b-ac78-6b4471e0d508"},"source":["## Training and Tuning"]},{"cell_type":"markdown","id":"8rnk3fyyNbB4","metadata":{"id":"8rnk3fyyNbB4"},"source":["> Here we define the hyperparameters of the neural network and begin training the network with those parameters. As deep learning is an iterative process- with model degredation and improvements both contributing to overall progress- this section does not contain the history of experimental training and parameter tuning that moonShot has undergone."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
